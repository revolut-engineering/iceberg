From aeeb978208ffc6ae2a462ddd68ab5c1e17703c7f Mon Sep 17 00:00:00 2001
From: Laith AlZyoud <46904854+laithalzyoud@users.noreply.github.com>
Date: Tue, 30 Jan 2024 14:58:58 +0100
Subject: [PATCH 1/5] Add actions to support copy table to another location
 (#1)

* Hive: Set commit state as Unknown before throwing CommitStateUnknownException (#7931) (#8029)

* Spark 3.4: WAP branch not propagated when using DELETE without WHERE (#7900) (#8028)

* Core: Include all reachable snapshots with v1 format and REF snapshot mode (#7621) (#8027)

* Spark 3.3: Backport 'WAP branch not propagated when using DELETE without WHERE' (#8033) (#8036)

* Flink: remove the creation of default database in FlinkCatalog open method (#7795) (#8039)

* Core: Handle optional fields (#8050) (#8064)

* Core: Handle allow optional fields

We expect:

- current-snapshot-id
- properties
- snapshots

to be there, but they are actually optional.

* Use AssertJ

* Core: Abort file groups should be under same lock as committerService (#7933) (#8060)

* Spark 3.4: Fix rewrite_position_deletes for certain partition types (#8059)

* Spark 3.3: Fix rewrite_position_deletes for certain partition types (#8059) (#8069)

* Spark: Add actions for disater recovery.

* Fix the compile error.

* Fix merge conflicts and formatting

* All tests are working and code integrated with Spark 3.3

* Fix union error and snapshots test

* Fix Spark broadcast error

* Add RewritePositionDeleteFilesSparkAction

---------

Co-authored-by: Eduard Tudenhoefner <etudenhoefner@gmail.com>
Co-authored-by: Fokko Driesprong <fokko@apache.org>
Co-authored-by: Xianyang Liu <liu-xianyang@hotmail.com>
Co-authored-by: Szehon Ho <szehon.apache@gmail.com>
Co-authored-by: Yufei Gu <yufei_gu@apple.com>
Co-authored-by: yufeigu <yufei@apache.org>
Co-authored-by: Laith Alzyoud <laith.alzyoud@revolut.com>
Co-authored-by: vaultah <4944562+vaultah@users.noreply.github.com>
---
 .../iceberg/actions/ActionsProvider.java      |  18 +
 .../actions/CheckSnapshotIntegrity.java       |  53 +
 .../org/apache/iceberg/actions/CopyTable.java |  88 ++
 .../iceberg/actions/RemoveExpiredFiles.java   |  58 ++
 .../org/apache/iceberg/ManifestEntry.java     |   2 +-
 .../org/apache/iceberg/ManifestFiles.java     |   8 +
 .../org/apache/iceberg/ManifestLists.java     |   6 +-
 .../org/apache/iceberg/ManifestReader.java    |   2 +-
 .../org/apache/iceberg/TableMetadataUtil.java | 130 +++
 .../BaseCheckSnapshotIntegrityResult.java     |  32 +
 .../actions/BaseCopyTableActionResult.java    |  47 +
 .../BaseRemoveExpiredFilesActionResult.java   |  47 +
 .../spark/actions/BaseSparkActions.java       |  61 ++
 .../spark/actions/BaseSparkAction.java        | 454 ---------
 .../iceberg/spark/actions/SparkActions.java   |  94 --
 ...BaseCheckSnapshotIntegritySparkAction.java | 147 +++
 .../actions/BaseCopyTableSparkAction.java     | 620 ++++++++++++
 .../BaseRemoveExpiredFilesSparkAction.java    | 189 ++++
 .../spark/actions/BaseSparkAction.java        | 155 ++-
 .../iceberg/spark/actions/SparkActions.java   |  18 +
 .../TestCheckSnapshotIntegrityAction.java     | 207 ++++
 .../spark/actions/TestCopyTableAction.java    | 918 ++++++++++++++++++
 .../actions/TestRemoveExpiredFilesAction.java | 233 +++++
 23 files changed, 3033 insertions(+), 554 deletions(-)
 create mode 100644 api/src/main/java/org/apache/iceberg/actions/CheckSnapshotIntegrity.java
 create mode 100644 api/src/main/java/org/apache/iceberg/actions/CopyTable.java
 create mode 100644 api/src/main/java/org/apache/iceberg/actions/RemoveExpiredFiles.java
 create mode 100644 core/src/main/java/org/apache/iceberg/TableMetadataUtil.java
 create mode 100644 core/src/main/java/org/apache/iceberg/actions/BaseCheckSnapshotIntegrityResult.java
 create mode 100644 core/src/main/java/org/apache/iceberg/actions/BaseCopyTableActionResult.java
 create mode 100644 core/src/main/java/org/apache/iceberg/actions/BaseRemoveExpiredFilesActionResult.java
 create mode 100644 spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkActions.java
 delete mode 100644 spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java
 delete mode 100644 spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java
 create mode 100644 spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCheckSnapshotIntegritySparkAction.java
 create mode 100644 spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
 create mode 100644 spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseRemoveExpiredFilesSparkAction.java
 create mode 100644 spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCheckSnapshotIntegrityAction.java
 create mode 100644 spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
 create mode 100644 spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveExpiredFilesAction.java

diff --git a/api/src/main/java/org/apache/iceberg/actions/ActionsProvider.java b/api/src/main/java/org/apache/iceberg/actions/ActionsProvider.java
index 2d6ff2679..72e49209e 100644
--- a/api/src/main/java/org/apache/iceberg/actions/ActionsProvider.java
+++ b/api/src/main/java/org/apache/iceberg/actions/ActionsProvider.java
@@ -70,4 +70,22 @@ public interface ActionsProvider {
     throw new UnsupportedOperationException(
         this.getClass().getName() + " does not implement rewritePositionDeletes");
   }
+
+  /** Instantiates an action to copy table. */
+  default CopyTable copyTable(Table table) {
+    throw new UnsupportedOperationException(
+        this.getClass().getName() + " does not implement copyTable");
+  }
+
+  /** Instantiates an action to check snapshot integrity. */
+  default CheckSnapshotIntegrity checkSnapshotIntegrity(Table table) {
+    throw new UnsupportedOperationException(
+        this.getClass().getName() + " does not implement checkSnapshotIntegrity");
+  }
+
+  /** Instantiates an action to remove expired files. */
+  default RemoveExpiredFiles removeExpiredFiles(Table table) {
+    throw new UnsupportedOperationException(
+        this.getClass().getName() + " does not implement removeExpiredFiles");
+  }
 }
diff --git a/api/src/main/java/org/apache/iceberg/actions/CheckSnapshotIntegrity.java b/api/src/main/java/org/apache/iceberg/actions/CheckSnapshotIntegrity.java
new file mode 100644
index 000000000..b3bea2233
--- /dev/null
+++ b/api/src/main/java/org/apache/iceberg/actions/CheckSnapshotIntegrity.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.actions;
+
+import java.util.concurrent.ExecutorService;
+
+public interface CheckSnapshotIntegrity
+    extends Action<CheckSnapshotIntegrity, CheckSnapshotIntegrity.Result> {
+
+  /**
+   * Passes an alternative executor service that will be used for snapshot integrity checking. If
+   * this method is not called, snapshot integrity checker will still be running by a single
+   * threaded executor service.
+   *
+   * @param executorService an executor service to parallelize tasks to check snapshot integrity
+   * @return this for method chaining
+   */
+  CheckSnapshotIntegrity executeWith(ExecutorService executorService);
+
+  /**
+   * Pass the target version to check. The action checks the snapshots in the target version, not in
+   * the current version of the table.
+   *
+   * @param targetVersion the target version file to be checked. Either a file name or a file path
+   *     is acceptable. For example, it could be either
+   *     "00001-8893aa9e-f92e-4443-80e7-cfa42238a654.metadata.json" or
+   *     "/path/to/00001-8893aa9e-f92e-4443-80e7-cfa42238a654.metadata.json".
+   * @return this for method chaining
+   */
+  CheckSnapshotIntegrity targetVersion(String targetVersion);
+
+  /** The action result that contains a summary of the execution. */
+  interface Result {
+    /** Returns locations of missing metadata/data files. */
+    Iterable<String> missingFileLocations();
+  }
+}
diff --git a/api/src/main/java/org/apache/iceberg/actions/CopyTable.java b/api/src/main/java/org/apache/iceberg/actions/CopyTable.java
new file mode 100644
index 000000000..245fec871
--- /dev/null
+++ b/api/src/main/java/org/apache/iceberg/actions/CopyTable.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.actions;
+
+import org.apache.iceberg.Table;
+
+public interface CopyTable extends Action<CopyTable, CopyTable.Result> {
+
+  /**
+   * Passes the source and target prefixes that will be used to replace the source prefix with the
+   * target one.
+   *
+   * @param sourcePrefix the source prefix to be replaced
+   * @param targetPrefix the target prefix
+   * @return this for method chaining
+   */
+  CopyTable rewriteLocationPrefix(String sourcePrefix, String targetPrefix);
+
+  /**
+   * Pass the version copied last time. It is optional if the target table is provided. The default
+   * value is the target table's current version. User needs to make sure whether the start version
+   * is valid if target table is not provided.
+   *
+   * @param lastCopiedVersion only version file name is needed, not the metadata json file path. For
+   *     example, the version file would be "v2.metadata.json" for a Hadoop table. For metastore
+   *     tables, the version file would be like
+   *     "00001-8893aa9e-f92e-4443-80e7-cfa42238a654.metadata.json".
+   * @return this for method chaining
+   */
+  CopyTable lastCopiedVersion(String lastCopiedVersion);
+
+  /**
+   * The latest version of the table to copy. It is optional, the default value is the source
+   * table's current version.
+   *
+   * @param endVersion only version file name is needed, not the metadata json file path. For
+   *     example, the version file would be "v2.metadata.json" for a Hadoop table. For metastore
+   *     tables, the version file would be like
+   *     "00001-8893aa9e-f92e-4443-80e7-cfa42238a654.metadata.json".
+   * @return this for method chaining
+   */
+  CopyTable endVersion(String endVersion);
+
+  /**
+   * Set the customized staging location. It is optional. By default, staging location is a sub
+   * directory under table's metadata directory.
+   *
+   * @param stagingLocation the staging location
+   * @return this for method chaining
+   */
+  CopyTable stagingLocation(String stagingLocation);
+
+  /**
+   * Set the target table. It is optional if the start version is provided.
+   *
+   * @param targetTable the target table
+   * @return this for method chaining
+   */
+  CopyTable targetTable(Table targetTable);
+
+  /** The action result that contains a summary of the execution. */
+  interface Result {
+    /** Return directory of data files list. */
+    String dataFileListLocation();
+
+    /** Return directory of metadata files list. */
+    String metadataFileListLocation();
+
+    /** Return the latest version */
+    String latestVersion();
+  }
+}
diff --git a/api/src/main/java/org/apache/iceberg/actions/RemoveExpiredFiles.java b/api/src/main/java/org/apache/iceberg/actions/RemoveExpiredFiles.java
new file mode 100644
index 000000000..4116c4fc0
--- /dev/null
+++ b/api/src/main/java/org/apache/iceberg/actions/RemoveExpiredFiles.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.actions;
+
+import java.util.concurrent.ExecutorService;
+
+public interface RemoveExpiredFiles extends Action<RemoveExpiredFiles, RemoveExpiredFiles.Result> {
+
+  /**
+   * Passes an alternative executor service that will be used for snapshot integrity checking. If
+   * this method is not called, snapshot integrity checker will still be running by a single
+   * threaded executor service.
+   *
+   * @param executorService an executor service to parallelize tasks to check snapshot integrity
+   * @return this for method chaining
+   */
+  RemoveExpiredFiles executeWith(ExecutorService executorService);
+
+  /**
+   * Pass the target version to check. The action checks the snapshots in the target version, not in
+   * the current version of the table.
+   *
+   * @param targetVersion the target version file to be checked. Either a file name or a file path
+   *     is acceptable. For example, it could be either
+   *     "00001-8893aa9e-f92e-4443-80e7-cfa42238a654.metadata.json" or
+   *     "/path/to/00001-8893aa9e-f92e-4443-80e7-cfa42238a654.metadata.json".
+   * @return this for method chaining
+   */
+  RemoveExpiredFiles targetVersion(String targetVersion);
+
+  /** The action result that contains a summary of the execution. */
+  interface Result {
+    /** Returns the number of deleted data files. */
+    long deletedDataFilesCount();
+
+    /** Returns the number of deleted manifests. */
+    long deletedManifestsCount();
+
+    /** Returns the number of deleted manifest lists. */
+    long deletedManifestListsCount();
+  }
+}
diff --git a/core/src/main/java/org/apache/iceberg/ManifestEntry.java b/core/src/main/java/org/apache/iceberg/ManifestEntry.java
index 3eeb5407a..b69ade2f8 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestEntry.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestEntry.java
@@ -24,7 +24,7 @@ import static org.apache.iceberg.types.Types.NestedField.required;
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.types.Types.StructType;
 
-interface ManifestEntry<F extends ContentFile<F>> {
+public interface ManifestEntry<F extends ContentFile<F>> {
   enum Status {
     EXISTING(0),
     ADDED(1),
diff --git a/core/src/main/java/org/apache/iceberg/ManifestFiles.java b/core/src/main/java/org/apache/iceberg/ManifestFiles.java
index c23ab667a..3803e07f8 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestFiles.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestFiles.java
@@ -35,6 +35,7 @@ import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTest
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
 import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
+import org.apache.iceberg.util.Pair;
 import org.apache.iceberg.util.PropertyUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -97,6 +98,13 @@ public class ManifestFiles {
         entry -> entry.file().path().toString());
   }
 
+  public static CloseableIterable<Pair<String, Long>> readPathsWithSnapshotId(
+      ManifestFile manifest, FileIO io) {
+    return CloseableIterable.transform(
+        read(manifest, io, null).select(ImmutableList.of("file_path", "snapshot_id")).liveEntries(),
+        entry -> Pair.of(entry.file().path().toString(), entry.snapshotId()));
+  }
+
   /**
    * Returns a new {@link ManifestReader} for a {@link ManifestFile}.
    *
diff --git a/core/src/main/java/org/apache/iceberg/ManifestLists.java b/core/src/main/java/org/apache/iceberg/ManifestLists.java
index c7b3e5fee..810926b3e 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestLists.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestLists.java
@@ -28,10 +28,10 @@ import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 
-class ManifestLists {
+public class ManifestLists {
   private ManifestLists() {}
 
-  static List<ManifestFile> read(InputFile manifestList) {
+  public static List<ManifestFile> read(InputFile manifestList) {
     try (CloseableIterable<ManifestFile> files =
         Avro.read(manifestList)
             .rename("manifest_file", GenericManifestFile.class.getName())
@@ -50,7 +50,7 @@ class ManifestLists {
     }
   }
 
-  static ManifestListWriter write(
+  public static ManifestListWriter write(
       int formatVersion,
       OutputFile manifestListFile,
       long snapshotId,
diff --git a/core/src/main/java/org/apache/iceberg/ManifestReader.java b/core/src/main/java/org/apache/iceberg/ManifestReader.java
index 4ee51aa60..cdb33128e 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestReader.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestReader.java
@@ -202,7 +202,7 @@ public class ManifestReader<F extends ContentFile<F>> extends CloseableGroup
     return this;
   }
 
-  CloseableIterable<ManifestEntry<F>> entries() {
+  public CloseableIterable<ManifestEntry<F>> entries() {
     return entries(false /* all entries */);
   }
 
diff --git a/core/src/main/java/org/apache/iceberg/TableMetadataUtil.java b/core/src/main/java/org/apache/iceberg/TableMetadataUtil.java
new file mode 100644
index 000000000..7d9a12f1d
--- /dev/null
+++ b/core/src/main/java/org/apache/iceberg/TableMetadataUtil.java
@@ -0,0 +1,130 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg;
+
+import java.util.List;
+import java.util.Map;
+import org.apache.iceberg.TableMetadata.MetadataLogEntry;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
+
+public class TableMetadataUtil {
+  private TableMetadataUtil() {}
+
+  public static TableMetadata replacePaths(
+      TableMetadata metadata, String sourcePrefix, String targetPrefix) {
+    String newLocation = newPath(metadata.location(), sourcePrefix, targetPrefix);
+    List<Snapshot> newSnapshots = updatePathInSnapshots(metadata, sourcePrefix, targetPrefix);
+    List<MetadataLogEntry> metadataLogEntries =
+        updatePathInMetadataLogs(metadata, sourcePrefix, targetPrefix);
+    long snapshotId =
+        metadata.currentSnapshot() == null ? -1 : metadata.currentSnapshot().snapshotId();
+    Map<String, String> properties =
+        updateProperties(metadata.properties(), sourcePrefix, targetPrefix);
+
+    return new TableMetadata(
+        null,
+        metadata.formatVersion(),
+        metadata.uuid(),
+        newLocation,
+        metadata.lastSequenceNumber(),
+        metadata.lastUpdatedMillis(),
+        metadata.lastColumnId(),
+        metadata.currentSchemaId(),
+        metadata.schemas(),
+        metadata.defaultSpecId(),
+        metadata.specs(),
+        metadata.lastAssignedPartitionId(),
+        metadata.defaultSortOrderId(),
+        metadata.sortOrders(),
+        properties,
+        snapshotId,
+        newSnapshots,
+        () -> newSnapshots,
+        metadata.snapshotLog(),
+        metadataLogEntries,
+        metadata.refs(),
+        metadata.statisticsFiles(),
+        metadata.changes());
+  }
+
+  private static Map<String, String> updateProperties(
+      Map<String, String> tableProperties, String sourcePrefix, String targetPrefix) {
+    Map properties = Maps.newHashMap(tableProperties);
+    updatePathInProperty(properties, sourcePrefix, targetPrefix, TableProperties.OBJECT_STORE_PATH);
+    updatePathInProperty(
+        properties, sourcePrefix, targetPrefix, TableProperties.WRITE_FOLDER_STORAGE_LOCATION);
+    updatePathInProperty(
+        properties, sourcePrefix, targetPrefix, TableProperties.WRITE_DATA_LOCATION);
+    updatePathInProperty(
+        properties, sourcePrefix, targetPrefix, TableProperties.WRITE_METADATA_LOCATION);
+
+    return properties;
+  }
+
+  private static void updatePathInProperty(
+      Map<String, String> properties,
+      String sourcePrefix,
+      String targetPrefix,
+      String propertyName) {
+    if (properties.containsKey(propertyName)) {
+      properties.put(
+          propertyName, newPath(properties.get(propertyName), sourcePrefix, targetPrefix));
+    }
+  }
+
+  private static List<MetadataLogEntry> updatePathInMetadataLogs(
+      TableMetadata metadata, String sourcePrefix, String targetPrefix) {
+    List<MetadataLogEntry> metadataLogEntries =
+        Lists.newArrayListWithCapacity(metadata.previousFiles().size());
+    for (MetadataLogEntry metadataLog : metadata.previousFiles()) {
+      MetadataLogEntry newMetadataLog =
+          new MetadataLogEntry(
+              metadataLog.timestampMillis(),
+              newPath(metadataLog.file(), sourcePrefix, targetPrefix));
+      metadataLogEntries.add(newMetadataLog);
+    }
+    return metadataLogEntries;
+  }
+
+  private static List<Snapshot> updatePathInSnapshots(
+      TableMetadata metadata, String sourcePrefix, String targetPrefix) {
+    List<Snapshot> newSnapshots = Lists.newArrayListWithCapacity(metadata.snapshots().size());
+    for (Snapshot snapshot : metadata.snapshots()) {
+      String newManifestListLocation =
+          newPath(snapshot.manifestListLocation(), sourcePrefix, targetPrefix);
+      Snapshot newSnapshot =
+          new BaseSnapshot(
+              snapshot.sequenceNumber(),
+              snapshot.snapshotId(),
+              snapshot.parentId(),
+              snapshot.timestampMillis(),
+              snapshot.operation(),
+              snapshot.summary(),
+              snapshot.schemaId(),
+              newManifestListLocation);
+      newSnapshots.add(newSnapshot);
+    }
+    return newSnapshots;
+  }
+
+  private static String newPath(String path, String sourcePrefix, String targetPrefix) {
+    return path.replaceFirst(sourcePrefix, targetPrefix);
+  }
+}
diff --git a/core/src/main/java/org/apache/iceberg/actions/BaseCheckSnapshotIntegrityResult.java b/core/src/main/java/org/apache/iceberg/actions/BaseCheckSnapshotIntegrityResult.java
new file mode 100644
index 000000000..b0660bca8
--- /dev/null
+++ b/core/src/main/java/org/apache/iceberg/actions/BaseCheckSnapshotIntegrityResult.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.actions;
+
+public class BaseCheckSnapshotIntegrityResult implements CheckSnapshotIntegrity.Result {
+  private final Iterable<String> missingFileLocations;
+
+  public BaseCheckSnapshotIntegrityResult(Iterable<String> missingFileLocations) {
+    this.missingFileLocations = missingFileLocations;
+  }
+
+  @Override
+  public Iterable<String> missingFileLocations() {
+    return missingFileLocations;
+  }
+}
diff --git a/core/src/main/java/org/apache/iceberg/actions/BaseCopyTableActionResult.java b/core/src/main/java/org/apache/iceberg/actions/BaseCopyTableActionResult.java
new file mode 100644
index 000000000..060b341f0
--- /dev/null
+++ b/core/src/main/java/org/apache/iceberg/actions/BaseCopyTableActionResult.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.actions;
+
+public class BaseCopyTableActionResult implements CopyTable.Result {
+  private final String latestVersion;
+  private final String dataFileListPath;
+  private final String metadataFileListPath;
+
+  public BaseCopyTableActionResult(
+      String dataFileListPath, String metadataFileListPath, String latestVersion) {
+    this.dataFileListPath = dataFileListPath;
+    this.metadataFileListPath = metadataFileListPath;
+    this.latestVersion = latestVersion;
+  }
+
+  @Override
+  public String dataFileListLocation() {
+    return dataFileListPath;
+  }
+
+  @Override
+  public String metadataFileListLocation() {
+    return metadataFileListPath;
+  }
+
+  @Override
+  public String latestVersion() {
+    return latestVersion;
+  }
+}
diff --git a/core/src/main/java/org/apache/iceberg/actions/BaseRemoveExpiredFilesActionResult.java b/core/src/main/java/org/apache/iceberg/actions/BaseRemoveExpiredFilesActionResult.java
new file mode 100644
index 000000000..9cf71768a
--- /dev/null
+++ b/core/src/main/java/org/apache/iceberg/actions/BaseRemoveExpiredFilesActionResult.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.actions;
+
+public class BaseRemoveExpiredFilesActionResult implements RemoveExpiredFiles.Result {
+  private final long deletedDataFilesCount;
+  private final long deletedManifestsCount;
+  private final long deletedManifestListsCount;
+
+  public BaseRemoveExpiredFilesActionResult(
+      long deletedDataFilesCount, long deletedManifestsCount, long deletedManifestListsCount) {
+    this.deletedDataFilesCount = deletedDataFilesCount;
+    this.deletedManifestsCount = deletedManifestsCount;
+    this.deletedManifestListsCount = deletedManifestListsCount;
+  }
+
+  @Override
+  public long deletedDataFilesCount() {
+    return deletedDataFilesCount;
+  }
+
+  @Override
+  public long deletedManifestsCount() {
+    return deletedManifestsCount;
+  }
+
+  @Override
+  public long deletedManifestListsCount() {
+    return deletedManifestListsCount;
+  }
+}
diff --git a/spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkActions.java b/spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkActions.java
new file mode 100644
index 000000000..58b57177c
--- /dev/null
+++ b/spark/v3.0/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkActions.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.spark.actions;
+
+import org.apache.iceberg.Table;
+import org.apache.iceberg.actions.ActionsProvider;
+import org.apache.iceberg.actions.DeleteOrphanFiles;
+import org.apache.iceberg.actions.DeleteReachableFiles;
+import org.apache.iceberg.actions.ExpireSnapshots;
+import org.apache.iceberg.actions.RewriteManifests;
+import org.apache.spark.sql.SparkSession;
+
+abstract class BaseSparkActions implements ActionsProvider {
+
+  private final SparkSession spark;
+
+  protected BaseSparkActions(SparkSession spark) {
+    this.spark = spark;
+  }
+
+  protected SparkSession spark() {
+    return spark;
+  }
+
+  @Override
+  public DeleteOrphanFiles deleteOrphanFiles(Table table) {
+    return new BaseDeleteOrphanFilesSparkAction(spark, table);
+  }
+
+  @Override
+  public RewriteManifests rewriteManifests(Table table) {
+    return new BaseRewriteManifestsSparkAction(spark, table);
+  }
+
+  @Override
+  public ExpireSnapshots expireSnapshots(Table table) {
+    return new BaseExpireSnapshotsSparkAction(spark, table);
+  }
+
+  @Override
+  public DeleteReachableFiles deleteReachableFiles(String metadataLocation) {
+    return new BaseDeleteReachableFilesSparkAction(spark, metadataLocation);
+  }
+}
diff --git a/spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java b/spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java
deleted file mode 100644
index 3c007c621..000000000
--- a/spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java
+++ /dev/null
@@ -1,454 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.iceberg.spark.actions;
-
-import static org.apache.iceberg.MetadataTableType.ALL_MANIFESTS;
-import static org.apache.spark.sql.functions.col;
-import static org.apache.spark.sql.functions.lit;
-
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicLong;
-import java.util.function.Consumer;
-import java.util.function.Predicate;
-import java.util.function.Supplier;
-import org.apache.iceberg.AllManifestsTable;
-import org.apache.iceberg.BaseTable;
-import org.apache.iceberg.ContentFile;
-import org.apache.iceberg.DataFile;
-import org.apache.iceberg.FileContent;
-import org.apache.iceberg.ManifestContent;
-import org.apache.iceberg.ManifestFiles;
-import org.apache.iceberg.MetadataTableType;
-import org.apache.iceberg.PartitionSpec;
-import org.apache.iceberg.ReachableFileUtil;
-import org.apache.iceberg.StaticTableOperations;
-import org.apache.iceberg.StatisticsFile;
-import org.apache.iceberg.Table;
-import org.apache.iceberg.TableMetadata;
-import org.apache.iceberg.exceptions.NotFoundException;
-import org.apache.iceberg.exceptions.ValidationException;
-import org.apache.iceberg.io.BulkDeletionFailureException;
-import org.apache.iceberg.io.CloseableIterator;
-import org.apache.iceberg.io.ClosingIterator;
-import org.apache.iceberg.io.FileIO;
-import org.apache.iceberg.io.SupportsBulkOperations;
-import org.apache.iceberg.relocated.com.google.common.base.Joiner;
-import org.apache.iceberg.relocated.com.google.common.base.Splitter;
-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
-import org.apache.iceberg.relocated.com.google.common.collect.Iterators;
-import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;
-import org.apache.iceberg.relocated.com.google.common.collect.Lists;
-import org.apache.iceberg.relocated.com.google.common.collect.Maps;
-import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;
-import org.apache.iceberg.spark.JobGroupInfo;
-import org.apache.iceberg.spark.JobGroupUtils;
-import org.apache.iceberg.spark.SparkTableUtil;
-import org.apache.iceberg.spark.source.SerializableTableWithSize;
-import org.apache.iceberg.util.Tasks;
-import org.apache.spark.SparkContext;
-import org.apache.spark.api.java.JavaSparkContext;
-import org.apache.spark.api.java.function.FlatMapFunction;
-import org.apache.spark.broadcast.Broadcast;
-import org.apache.spark.sql.Column;
-import org.apache.spark.sql.Dataset;
-import org.apache.spark.sql.Row;
-import org.apache.spark.sql.SparkSession;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-abstract class BaseSparkAction<ThisT> {
-
-  protected static final String MANIFEST = "Manifest";
-  protected static final String MANIFEST_LIST = "Manifest List";
-  protected static final String STATISTICS_FILES = "Statistics Files";
-  protected static final String OTHERS = "Others";
-
-  protected static final String FILE_PATH = "file_path";
-  protected static final String LAST_MODIFIED = "last_modified";
-
-  protected static final Splitter COMMA_SPLITTER = Splitter.on(",");
-  protected static final Joiner COMMA_JOINER = Joiner.on(',');
-
-  private static final Logger LOG = LoggerFactory.getLogger(BaseSparkAction.class);
-  private static final AtomicInteger JOB_COUNTER = new AtomicInteger();
-  private static final int DELETE_NUM_RETRIES = 3;
-  private static final int DELETE_GROUP_SIZE = 100000;
-
-  private final SparkSession spark;
-  private final JavaSparkContext sparkContext;
-  private final Map<String, String> options = Maps.newHashMap();
-
-  protected BaseSparkAction(SparkSession spark) {
-    this.spark = spark;
-    this.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());
-  }
-
-  protected SparkSession spark() {
-    return spark;
-  }
-
-  protected JavaSparkContext sparkContext() {
-    return sparkContext;
-  }
-
-  protected abstract ThisT self();
-
-  public ThisT option(String name, String value) {
-    options.put(name, value);
-    return self();
-  }
-
-  public ThisT options(Map<String, String> newOptions) {
-    options.putAll(newOptions);
-    return self();
-  }
-
-  protected Map<String, String> options() {
-    return options;
-  }
-
-  protected <T> T withJobGroupInfo(JobGroupInfo info, Supplier<T> supplier) {
-    SparkContext context = spark().sparkContext();
-    JobGroupInfo previousInfo = JobGroupUtils.getJobGroupInfo(context);
-    try {
-      JobGroupUtils.setJobGroupInfo(context, info);
-      return supplier.get();
-    } finally {
-      JobGroupUtils.setJobGroupInfo(context, previousInfo);
-    }
-  }
-
-  protected JobGroupInfo newJobGroupInfo(String groupId, String desc) {
-    return new JobGroupInfo(groupId + "-" + JOB_COUNTER.incrementAndGet(), desc, false);
-  }
-
-  protected Table newStaticTable(TableMetadata metadata, FileIO io) {
-    String metadataFileLocation = metadata.metadataFileLocation();
-    StaticTableOperations ops = new StaticTableOperations(metadataFileLocation, io);
-    return new BaseTable(ops, metadataFileLocation);
-  }
-
-  protected Dataset<FileInfo> contentFileDS(Table table) {
-    return contentFileDS(table, null);
-  }
-
-  protected Dataset<FileInfo> contentFileDS(Table table, Set<Long> snapshotIds) {
-    Table serializableTable = SerializableTableWithSize.copyOf(table);
-    Broadcast<Table> tableBroadcast = sparkContext.broadcast(serializableTable);
-    int numShufflePartitions = spark.sessionState().conf().numShufflePartitions();
-
-    Dataset<ManifestFileBean> manifestBeanDS =
-        manifestDF(table, snapshotIds)
-            .selectExpr(
-                "content",
-                "path",
-                "length",
-                "partition_spec_id as partitionSpecId",
-                "added_snapshot_id as addedSnapshotId")
-            .dropDuplicates("path")
-            .repartition(numShufflePartitions) // avoid adaptive execution combining tasks
-            .as(ManifestFileBean.ENCODER);
-
-    return manifestBeanDS.flatMap(new ReadManifest(tableBroadcast), FileInfo.ENCODER);
-  }
-
-  protected Dataset<FileInfo> manifestDS(Table table) {
-    return manifestDS(table, null);
-  }
-
-  protected Dataset<FileInfo> manifestDS(Table table, Set<Long> snapshotIds) {
-    return manifestDF(table, snapshotIds)
-        .select(col("path"), lit(MANIFEST).as("type"))
-        .as(FileInfo.ENCODER);
-  }
-
-  private Dataset<Row> manifestDF(Table table, Set<Long> snapshotIds) {
-    Dataset<Row> manifestDF = loadMetadataTable(table, ALL_MANIFESTS);
-    if (snapshotIds != null) {
-      Column filterCond = col(AllManifestsTable.REF_SNAPSHOT_ID.name()).isInCollection(snapshotIds);
-      return manifestDF.filter(filterCond);
-    } else {
-      return manifestDF;
-    }
-  }
-
-  protected Dataset<FileInfo> manifestListDS(Table table) {
-    return manifestListDS(table, null);
-  }
-
-  protected Dataset<FileInfo> manifestListDS(Table table, Set<Long> snapshotIds) {
-    List<String> manifestLists = ReachableFileUtil.manifestListLocations(table, snapshotIds);
-    return toFileInfoDS(manifestLists, MANIFEST_LIST);
-  }
-
-  protected Dataset<FileInfo> statisticsFileDS(Table table, Set<Long> snapshotIds) {
-    Predicate<StatisticsFile> predicate;
-    if (snapshotIds == null) {
-      predicate = statisticsFile -> true;
-    } else {
-      predicate = statisticsFile -> snapshotIds.contains(statisticsFile.snapshotId());
-    }
-
-    List<String> statisticsFiles = ReachableFileUtil.statisticsFilesLocations(table, predicate);
-    return toFileInfoDS(statisticsFiles, STATISTICS_FILES);
-  }
-
-  protected Dataset<FileInfo> otherMetadataFileDS(Table table) {
-    return otherMetadataFileDS(table, false /* include all reachable old metadata locations */);
-  }
-
-  protected Dataset<FileInfo> allReachableOtherMetadataFileDS(Table table) {
-    return otherMetadataFileDS(table, true /* include all reachable old metadata locations */);
-  }
-
-  private Dataset<FileInfo> otherMetadataFileDS(Table table, boolean recursive) {
-    List<String> otherMetadataFiles = Lists.newArrayList();
-    otherMetadataFiles.addAll(ReachableFileUtil.metadataFileLocations(table, recursive));
-    otherMetadataFiles.add(ReachableFileUtil.versionHintLocation(table));
-    otherMetadataFiles.addAll(ReachableFileUtil.statisticsFilesLocations(table));
-    return toFileInfoDS(otherMetadataFiles, OTHERS);
-  }
-
-  protected Dataset<Row> loadMetadataTable(Table table, MetadataTableType type) {
-    return SparkTableUtil.loadMetadataTable(spark, table, type);
-  }
-
-  private Dataset<FileInfo> toFileInfoDS(List<String> paths, String type) {
-    List<FileInfo> fileInfoList = Lists.transform(paths, path -> new FileInfo(path, type));
-    return spark.createDataset(fileInfoList, FileInfo.ENCODER);
-  }
-
-  /**
-   * Deletes files and keeps track of how many files were removed for each file type.
-   *
-   * @param executorService an executor service to use for parallel deletes
-   * @param deleteFunc a delete func
-   * @param files an iterator of Spark rows of the structure (path: String, type: String)
-   * @return stats on which files were deleted
-   */
-  protected DeleteSummary deleteFiles(
-      ExecutorService executorService, Consumer<String> deleteFunc, Iterator<FileInfo> files) {
-
-    DeleteSummary summary = new DeleteSummary();
-
-    Tasks.foreach(files)
-        .retry(DELETE_NUM_RETRIES)
-        .stopRetryOn(NotFoundException.class)
-        .suppressFailureWhenFinished()
-        .executeWith(executorService)
-        .onFailure(
-            (fileInfo, exc) -> {
-              String path = fileInfo.getPath();
-              String type = fileInfo.getType();
-              LOG.warn("Delete failed for {}: {}", type, path, exc);
-            })
-        .run(
-            fileInfo -> {
-              String path = fileInfo.getPath();
-              String type = fileInfo.getType();
-              deleteFunc.accept(path);
-              summary.deletedFile(path, type);
-            });
-
-    return summary;
-  }
-
-  protected DeleteSummary deleteFiles(SupportsBulkOperations io, Iterator<FileInfo> files) {
-    DeleteSummary summary = new DeleteSummary();
-    Iterator<List<FileInfo>> fileGroups = Iterators.partition(files, DELETE_GROUP_SIZE);
-
-    Tasks.foreach(fileGroups)
-        .suppressFailureWhenFinished()
-        .run(fileGroup -> deleteFileGroup(fileGroup, io, summary));
-
-    return summary;
-  }
-
-  private static void deleteFileGroup(
-      List<FileInfo> fileGroup, SupportsBulkOperations io, DeleteSummary summary) {
-
-    ListMultimap<String, FileInfo> filesByType = Multimaps.index(fileGroup, FileInfo::getType);
-    ListMultimap<String, String> pathsByType =
-        Multimaps.transformValues(filesByType, FileInfo::getPath);
-
-    for (Map.Entry<String, Collection<String>> entry : pathsByType.asMap().entrySet()) {
-      String type = entry.getKey();
-      Collection<String> paths = entry.getValue();
-      int failures = 0;
-      try {
-        io.deleteFiles(paths);
-      } catch (BulkDeletionFailureException e) {
-        failures = e.numberFailedObjects();
-      }
-      summary.deletedFiles(type, paths.size() - failures);
-    }
-  }
-
-  static class DeleteSummary {
-    private final AtomicLong dataFilesCount = new AtomicLong(0L);
-    private final AtomicLong positionDeleteFilesCount = new AtomicLong(0L);
-    private final AtomicLong equalityDeleteFilesCount = new AtomicLong(0L);
-    private final AtomicLong manifestsCount = new AtomicLong(0L);
-    private final AtomicLong manifestListsCount = new AtomicLong(0L);
-    private final AtomicLong statisticsFilesCount = new AtomicLong(0L);
-    private final AtomicLong otherFilesCount = new AtomicLong(0L);
-
-    public void deletedFiles(String type, int numFiles) {
-      if (FileContent.DATA.name().equalsIgnoreCase(type)) {
-        dataFilesCount.addAndGet(numFiles);
-
-      } else if (FileContent.POSITION_DELETES.name().equalsIgnoreCase(type)) {
-        positionDeleteFilesCount.addAndGet(numFiles);
-
-      } else if (FileContent.EQUALITY_DELETES.name().equalsIgnoreCase(type)) {
-        equalityDeleteFilesCount.addAndGet(numFiles);
-
-      } else if (MANIFEST.equalsIgnoreCase(type)) {
-        manifestsCount.addAndGet(numFiles);
-
-      } else if (MANIFEST_LIST.equalsIgnoreCase(type)) {
-        manifestListsCount.addAndGet(numFiles);
-
-      } else if (STATISTICS_FILES.equalsIgnoreCase(type)) {
-        statisticsFilesCount.addAndGet(numFiles);
-
-      } else if (OTHERS.equalsIgnoreCase(type)) {
-        otherFilesCount.addAndGet(numFiles);
-
-      } else {
-        throw new ValidationException("Illegal file type: %s", type);
-      }
-    }
-
-    public void deletedFile(String path, String type) {
-      if (FileContent.DATA.name().equalsIgnoreCase(type)) {
-        dataFilesCount.incrementAndGet();
-        LOG.trace("Deleted data file: {}", path);
-
-      } else if (FileContent.POSITION_DELETES.name().equalsIgnoreCase(type)) {
-        positionDeleteFilesCount.incrementAndGet();
-        LOG.trace("Deleted positional delete file: {}", path);
-
-      } else if (FileContent.EQUALITY_DELETES.name().equalsIgnoreCase(type)) {
-        equalityDeleteFilesCount.incrementAndGet();
-        LOG.trace("Deleted equality delete file: {}", path);
-
-      } else if (MANIFEST.equalsIgnoreCase(type)) {
-        manifestsCount.incrementAndGet();
-        LOG.debug("Deleted manifest: {}", path);
-
-      } else if (MANIFEST_LIST.equalsIgnoreCase(type)) {
-        manifestListsCount.incrementAndGet();
-        LOG.debug("Deleted manifest list: {}", path);
-
-      } else if (STATISTICS_FILES.equalsIgnoreCase(type)) {
-        statisticsFilesCount.incrementAndGet();
-        LOG.debug("Deleted statistics file: {}", path);
-
-      } else if (OTHERS.equalsIgnoreCase(type)) {
-        otherFilesCount.incrementAndGet();
-        LOG.debug("Deleted other metadata file: {}", path);
-
-      } else {
-        throw new ValidationException("Illegal file type: %s", type);
-      }
-    }
-
-    public long dataFilesCount() {
-      return dataFilesCount.get();
-    }
-
-    public long positionDeleteFilesCount() {
-      return positionDeleteFilesCount.get();
-    }
-
-    public long equalityDeleteFilesCount() {
-      return equalityDeleteFilesCount.get();
-    }
-
-    public long manifestsCount() {
-      return manifestsCount.get();
-    }
-
-    public long manifestListsCount() {
-      return manifestListsCount.get();
-    }
-
-    public long statisticsFilesCount() {
-      return statisticsFilesCount.get();
-    }
-
-    public long otherFilesCount() {
-      return otherFilesCount.get();
-    }
-
-    public long totalFilesCount() {
-      return dataFilesCount()
-          + positionDeleteFilesCount()
-          + equalityDeleteFilesCount()
-          + manifestsCount()
-          + manifestListsCount()
-          + statisticsFilesCount()
-          + otherFilesCount();
-    }
-  }
-
-  private static class ReadManifest implements FlatMapFunction<ManifestFileBean, FileInfo> {
-    private final Broadcast<Table> table;
-
-    ReadManifest(Broadcast<Table> table) {
-      this.table = table;
-    }
-
-    @Override
-    public Iterator<FileInfo> call(ManifestFileBean manifest) {
-      return new ClosingIterator<>(entries(manifest));
-    }
-
-    public CloseableIterator<FileInfo> entries(ManifestFileBean manifest) {
-      ManifestContent content = manifest.content();
-      FileIO io = table.getValue().io();
-      Map<Integer, PartitionSpec> specs = table.getValue().specs();
-      List<String> proj = ImmutableList.of(DataFile.FILE_PATH.name(), DataFile.CONTENT.name());
-
-      switch (content) {
-        case DATA:
-          return CloseableIterator.transform(
-              ManifestFiles.read(manifest, io, specs).select(proj).iterator(),
-              ReadManifest::toFileInfo);
-        case DELETES:
-          return CloseableIterator.transform(
-              ManifestFiles.readDeleteManifest(manifest, io, specs).select(proj).iterator(),
-              ReadManifest::toFileInfo);
-        default:
-          throw new IllegalArgumentException("Unsupported manifest content type:" + content);
-      }
-    }
-
-    static FileInfo toFileInfo(ContentFile<?> file) {
-      return new FileInfo(file.path().toString(), file.content().toString());
-    }
-  }
-}
diff --git a/spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java b/spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java
deleted file mode 100644
index 8c886adf5..000000000
--- a/spark/v3.2/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.iceberg.spark.actions;
-
-import org.apache.iceberg.Table;
-import org.apache.iceberg.actions.ActionsProvider;
-import org.apache.iceberg.spark.Spark3Util;
-import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;
-import org.apache.spark.sql.SparkSession;
-import org.apache.spark.sql.connector.catalog.CatalogPlugin;
-
-/**
- * An implementation of {@link ActionsProvider} for Spark.
- *
- * <p>This class is the primary API for interacting with actions in Spark that users should use to
- * instantiate particular actions.
- */
-public class SparkActions implements ActionsProvider {
-
-  private final SparkSession spark;
-
-  private SparkActions(SparkSession spark) {
-    this.spark = spark;
-  }
-
-  public static SparkActions get(SparkSession spark) {
-    return new SparkActions(spark);
-  }
-
-  public static SparkActions get() {
-    return new SparkActions(SparkSession.active());
-  }
-
-  @Override
-  public SnapshotTableSparkAction snapshotTable(String tableIdent) {
-    String ctx = "snapshot source";
-    CatalogPlugin defaultCatalog = spark.sessionState().catalogManager().currentCatalog();
-    CatalogAndIdentifier catalogAndIdent =
-        Spark3Util.catalogAndIdentifier(ctx, spark, tableIdent, defaultCatalog);
-    return new SnapshotTableSparkAction(
-        spark, catalogAndIdent.catalog(), catalogAndIdent.identifier());
-  }
-
-  @Override
-  public MigrateTableSparkAction migrateTable(String tableIdent) {
-    String ctx = "migrate target";
-    CatalogPlugin defaultCatalog = spark.sessionState().catalogManager().currentCatalog();
-    CatalogAndIdentifier catalogAndIdent =
-        Spark3Util.catalogAndIdentifier(ctx, spark, tableIdent, defaultCatalog);
-    return new MigrateTableSparkAction(
-        spark, catalogAndIdent.catalog(), catalogAndIdent.identifier());
-  }
-
-  @Override
-  public RewriteDataFilesSparkAction rewriteDataFiles(Table table) {
-    return new RewriteDataFilesSparkAction(spark, table);
-  }
-
-  @Override
-  public DeleteOrphanFilesSparkAction deleteOrphanFiles(Table table) {
-    return new DeleteOrphanFilesSparkAction(spark, table);
-  }
-
-  @Override
-  public RewriteManifestsSparkAction rewriteManifests(Table table) {
-    return new RewriteManifestsSparkAction(spark, table);
-  }
-
-  @Override
-  public ExpireSnapshotsSparkAction expireSnapshots(Table table) {
-    return new ExpireSnapshotsSparkAction(spark, table);
-  }
-
-  @Override
-  public DeleteReachableFilesSparkAction deleteReachableFiles(String metadataLocation) {
-    return new DeleteReachableFilesSparkAction(spark, metadataLocation);
-  }
-}
diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCheckSnapshotIntegritySparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCheckSnapshotIntegritySparkAction.java
new file mode 100644
index 000000000..7bb9af26e
--- /dev/null
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCheckSnapshotIntegritySparkAction.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.spark.actions;
+
+import java.io.File;
+import java.util.Collections;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.ExecutorService;
+import java.util.function.Consumer;
+import org.apache.iceberg.HasTableOperations;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.actions.BaseCheckSnapshotIntegrityResult;
+import org.apache.iceberg.actions.CheckSnapshotIntegrity;
+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
+import org.apache.iceberg.relocated.com.google.common.collect.Sets;
+import org.apache.iceberg.spark.JobGroupInfo;
+import org.apache.iceberg.util.Tasks;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Encoders;
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.SparkSession;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class BaseCheckSnapshotIntegritySparkAction extends BaseSparkAction<CheckSnapshotIntegrity>
+    implements CheckSnapshotIntegrity {
+
+  private static final Logger LOG =
+      LoggerFactory.getLogger(BaseCheckSnapshotIntegritySparkAction.class);
+  private static final ExecutorService DEFAULT_EXECUTOR_SERVICE = null;
+
+  private final Table table;
+  private final Set<String> missingFiles = Collections.synchronizedSet(Sets.newHashSet());
+  private ExecutorService executorService = DEFAULT_EXECUTOR_SERVICE;
+  private String targetVersion;
+  private Table targetTable;
+
+  private Consumer<String> validateFunc =
+      new Consumer<String>() {
+        @Override
+        public void accept(String file) {
+          try {
+            if (!table.io().newInputFile(file).exists()) {
+              missingFiles.add(file);
+            }
+          } catch (Exception e) {
+            LOG.warn("Failed to check the existence of file: {}. Marking it as missing.", file, e);
+            missingFiles.add(file);
+          }
+        }
+      };
+
+  public BaseCheckSnapshotIntegritySparkAction(SparkSession spark, Table table) {
+    super(spark);
+    this.table = table;
+  }
+
+  @Override
+  protected CheckSnapshotIntegrity self() {
+    return this;
+  }
+
+  @Override
+  public CheckSnapshotIntegrity executeWith(ExecutorService service) {
+    this.executorService = service;
+    return this;
+  }
+
+  @Override
+  public CheckSnapshotIntegrity targetVersion(String tVersion) {
+    Preconditions.checkArgument(
+        tVersion != null && !tVersion.isEmpty(),
+        "Target version file('%s') cannot be empty.",
+        tVersion);
+
+    String tVersionFile = tVersion;
+    if (!tVersionFile.contains(File.separator)) {
+      tVersionFile = ((HasTableOperations) table).operations().metadataFileLocation(tVersionFile);
+    }
+
+    Preconditions.checkArgument(
+        fileExist(tVersionFile), "Version file('%s') doesn't exist.", tVersionFile);
+    this.targetVersion = tVersionFile;
+    return this;
+  }
+
+  @Override
+  public Result execute() {
+    JobGroupInfo info = newJobGroupInfo("CHECK-SNAPSHOT-INTEGRITY", jobDesc());
+    return withJobGroupInfo(info, this::doExecute);
+  }
+
+  private String jobDesc() {
+    return String.format(
+        "Checking integrity of version '%s' of table %s.", targetVersion, table.name());
+  }
+
+  private CheckSnapshotIntegrity.Result doExecute() {
+    targetTable = newStaticTable(targetVersion, table.io());
+
+    List<String> filesToCheck = filesToCheck();
+
+    Tasks.foreach(filesToCheck)
+        .noRetry()
+        .suppressFailureWhenFinished()
+        .executeWith(executorService)
+        .run(validateFunc::accept);
+
+    return new BaseCheckSnapshotIntegrityResult(missingFiles);
+  }
+
+  private List<String> filesToCheck() {
+    Dataset<Row> targetFileDF = fileDF(targetTable);
+    Dataset<Row> currentFileDF = fileDF(table);
+    return targetFileDF.except(currentFileDF).as(Encoders.STRING()).collectAsList();
+  }
+
+  private Dataset<Row> fileDF(Table tbl) {
+    Dataset<Row> validDataFileDF = buildValidDataFileDF(tbl);
+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF(tbl);
+    return validDataFileDF.union(validMetadataFileDF);
+  }
+
+  private boolean fileExist(String path) {
+    if (path == null || path.trim().isEmpty()) {
+      return false;
+    }
+    return table.io().newInputFile(path).exists();
+  }
+}
diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
new file mode 100644
index 000000000..f68304a14
--- /dev/null
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
@@ -0,0 +1,620 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.spark.actions;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.UncheckedIOException;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+import org.apache.iceberg.DataFile;
+import org.apache.iceberg.DataFiles;
+import org.apache.iceberg.HasTableOperations;
+import org.apache.iceberg.ManifestEntry;
+import org.apache.iceberg.ManifestFile;
+import org.apache.iceberg.ManifestFiles;
+import org.apache.iceberg.ManifestLists;
+import org.apache.iceberg.ManifestReader;
+import org.apache.iceberg.ManifestWriter;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.SerializableTable;
+import org.apache.iceberg.Snapshot;
+import org.apache.iceberg.StaticTableOperations;
+import org.apache.iceberg.StructLike;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableMetadata;
+import org.apache.iceberg.TableMetadata.MetadataLogEntry;
+import org.apache.iceberg.TableMetadataParser;
+import org.apache.iceberg.TableMetadataUtil;
+import org.apache.iceberg.actions.BaseCopyTableActionResult;
+import org.apache.iceberg.actions.CopyTable;
+import org.apache.iceberg.exceptions.RuntimeIOException;
+import org.apache.iceberg.io.FileAppender;
+import org.apache.iceberg.io.FileIO;
+import org.apache.iceberg.io.OutputFile;
+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
+import org.apache.iceberg.relocated.com.google.common.collect.Sets;
+import org.apache.iceberg.spark.JobGroupInfo;
+import org.apache.spark.api.java.function.MapPartitionsFunction;
+import org.apache.spark.broadcast.Broadcast;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Encoder;
+import org.apache.spark.sql.Encoders;
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.SaveMode;
+import org.apache.spark.sql.SparkSession;
+import org.apache.spark.sql.functions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> implements CopyTable {
+
+  private static final Logger LOG = LoggerFactory.getLogger(BaseCopyTableSparkAction.class);
+  private static final String DATA_FILE_LIST_DIR = "data-file-list-to-move";
+  private static final String METADATA_FILE_LIST_DIR = "metadata-file-list-to-move";
+
+  private final Table table;
+  private final Set<String> metadataFilesToMove = Collections.synchronizedSet(Sets.newHashSet());
+  private final Set<String> manifestFilePaths = Collections.synchronizedSet(Sets.newHashSet());
+  private final Set<ManifestFile> manifestFilesToRewrite =
+      Collections.synchronizedSet(Sets.newHashSet());
+  private String dataFileListPath = null;
+  private String metadataFileListPath = null;
+  private final boolean enabledPME;
+
+  private String sourcePrefix = "";
+  private String targetPrefix = "";
+  private String startVersion = "";
+  private String endVersion = "";
+  private String stagingDir = "";
+  private Table targetTable = null;
+
+  private Table startStaticTable = null;
+  private Table endStaticTable = null;
+
+  public BaseCopyTableSparkAction(SparkSession spark, Table table) {
+    super(spark);
+    this.table = table;
+    enabledPME = table.properties().containsKey("parquet.encryption.footer.key");
+  }
+
+  @Override
+  public CopyTable rewriteLocationPrefix(String sPrefix, String tPrefix) {
+    Preconditions.checkArgument(
+        sPrefix != null && !sPrefix.isEmpty(), "Source prefix('%s') cannot be empty.", sPrefix);
+    this.sourcePrefix = sPrefix;
+
+    if (tPrefix != null) {
+      this.targetPrefix = tPrefix;
+    }
+    return this;
+  }
+
+  @Override
+  public CopyTable lastCopiedVersion(String sVersion) {
+    Preconditions.checkArgument(
+        sVersion != null && !sVersion.trim().isEmpty(),
+        "Last copied version('%s') cannot be empty.",
+        sVersion);
+    this.startVersion = sVersion;
+    return this;
+  }
+
+  @Override
+  public CopyTable endVersion(String eVersion) {
+    Preconditions.checkArgument(
+        eVersion != null && !eVersion.trim().isEmpty(),
+        "End version('%s') cannot be empty.",
+        eVersion);
+    this.endVersion = eVersion;
+    return this;
+  }
+
+  @Override
+  public CopyTable stagingLocation(String stagingLocation) {
+    Preconditions.checkArgument(
+        stagingLocation != null && !stagingLocation.isEmpty(),
+        "Staging location('%s') cannot be empty.",
+        stagingLocation);
+    this.stagingDir = stagingLocation;
+    return this;
+  }
+
+  @Override
+  public CopyTable targetTable(Table tgtTable) {
+    this.targetTable = tgtTable;
+    return this;
+  }
+
+  @Override
+  protected CopyTable self() {
+    return this;
+  }
+
+  @Override
+  public Result execute() {
+    validateInputs();
+    JobGroupInfo info = newJobGroupInfo("COPY-TABLE", jobDesc());
+    return withJobGroupInfo(info, this::doExecute);
+  }
+
+  private CopyTable.Result doExecute() {
+    rebuildMetaData();
+    return new BaseCopyTableActionResult(
+        dataFileListPath, metadataFileListPath, fileName(endVersion));
+  }
+
+  private void validateInputs() {
+    Preconditions.checkArgument(
+        sourcePrefix != null && !sourcePrefix.isEmpty(),
+        "Source prefix('%s') cannot be empty.",
+        sourcePrefix);
+
+    validateAndSetEndVersion();
+
+    endStaticTable = newStaticTable(endVersion, table.io());
+
+    TableMetadata tableMetadata = ((HasTableOperations) endStaticTable).operations().current();
+    Preconditions.checkArgument(
+        tableMetadata.formatVersion() == 2, "Support Iceberg format version 2 only.");
+
+    validateAndSetStartVersion(tableMetadata);
+
+    if (fileExist(startVersion)) {
+      startStaticTable = newStaticTable(startVersion, table.io());
+    }
+
+    if (stagingDir.isEmpty()) {
+      stagingDir = getMetadataLocation(table) + "copy-table-staging-" + UUID.randomUUID() + "/";
+    } else if (!stagingDir.endsWith("/")) {
+      stagingDir = stagingDir + "/";
+    }
+  }
+
+  private void validateAndSetEndVersion() {
+    if (endVersion.isEmpty()) {
+      endVersion = currentMetadataPath(table);
+    } else {
+      TableMetadata tableMetadata = ((HasTableOperations) table).operations().current();
+      if (versionInFilePath(tableMetadata.metadataFileLocation(), endVersion)) {
+        endVersion = tableMetadata.metadataFileLocation();
+      }
+      for (MetadataLogEntry metadataLogEntry : tableMetadata.previousFiles()) {
+        if (versionInFilePath(metadataLogEntry.file(), endVersion)) {
+          endVersion = metadataLogEntry.file();
+          break;
+        }
+      }
+
+      Preconditions.checkArgument(
+          fileExist(endVersion),
+          "Cannot find the end version('%s') in the current version " + "files",
+          endVersion);
+    }
+  }
+
+  private void validateAndSetStartVersion(TableMetadata tableMetadata) {
+    if (startVersion.isEmpty()) {
+      if (targetTable == null) {
+        LOG.warn("No input of the start version. Will do a full copy.");
+      } else {
+        String tgtTableCurrentVersion = fileName(currentMetadataPath(targetTable));
+
+        for (MetadataLogEntry metadataLogEntry : tableMetadata.previousFiles()) {
+          if (metadataLogEntry.file().endsWith(tgtTableCurrentVersion)) {
+            startVersion = metadataLogEntry.file();
+            break;
+          }
+        }
+
+        if (fileNotExist(startVersion)) {
+          throw new IllegalArgumentException(
+              "Cannot find the current version of target table in the source table. "
+                  + "Please make sure the target table is a subset of source table.");
+        }
+      }
+    } else {
+      for (MetadataLogEntry metadataLogEntry : tableMetadata.previousFiles()) {
+        if (versionInFilePath(metadataLogEntry.file(), startVersion)) {
+          startVersion = metadataLogEntry.file();
+          break;
+        }
+      }
+
+      Preconditions.checkArgument(
+          fileExist(startVersion), "Start version('%s') is NOT valid.", startVersion);
+
+      if (targetTable != null
+          && !fileName(startVersion).equals(fileName(currentMetadataPath(targetTable)))) {
+        throw new IllegalArgumentException(
+            "The start version isn't the current version of the target table. "
+                + "Please make sure the target table is a subset of source table.");
+      }
+    }
+  }
+
+  private boolean versionInFilePath(String path, String version) {
+    return fileName(path).equals(version);
+  }
+
+  private String jobDesc() {
+    if (startVersion.isEmpty()) {
+      return String.format(
+          "Replacing path prefixes '%s' with '%s' in the metadata files of table %s,"
+              + "up to version '%s'.",
+          sourcePrefix, targetPrefix, table.name(), endVersion);
+    } else {
+      return String.format(
+          "Replacing path prefixes '%s' with '%s' in the metadata files of table %s,"
+              + "from version '%s' to '%s'.",
+          sourcePrefix, targetPrefix, table.name(), startVersion, endVersion);
+    }
+  }
+
+  /**
+   * Here are steps: 1. rebuild version files 2. rebuild manifest list files 3. rebuild manifest
+   * files 4. get all data files need to move
+   */
+  private void rebuildMetaData() {
+    TableMetadata tableMetadata = ((HasTableOperations) endStaticTable).operations().current();
+
+    // rebuild version files
+    Set<Long> allSnapshotIds = rewriteVersionFiles(tableMetadata);
+
+    Set<Long> diffSnapshotIds = getDiffSnapshotIds(allSnapshotIds);
+
+    // get all manifest file paths need to rewrite
+    List<String> manifestFilePathToMove = manifestFilesToMove(diffSnapshotIds);
+    manifestFilePaths.addAll(manifestFilePathToMove);
+
+    // rebuild manifest-list files
+    Set<Snapshot> validSnapshots =
+        Sets.difference(snapshotSet(endVersion), snapshotSet(startVersion));
+    validSnapshots.forEach(snapshot -> rewriteManifestList(snapshot, tableMetadata));
+
+    // rebuild manifest files
+    List<ManifestFile> newManifests = rewriteManifests(tableMetadata);
+    newManifests.forEach(manifestFile -> addToRebuiltFiles(manifestFile.path()));
+    saveMetadataFileList();
+
+    Dataset<Row> dataFiles = getDiffDataFiles(diffSnapshotIds);
+    saveDataFileList(dataFiles);
+  }
+
+  private void saveMetadataFileList() {
+    List<String> fileList = Lists.newArrayList();
+    fileList.addAll(metadataFilesToMove);
+    Dataset<String> metadataFileList = spark().createDataset(fileList, Encoders.STRING());
+    metadataFileListPath = stagingDir + METADATA_FILE_LIST_DIR;
+    metadataFileList
+        .repartition(1)
+        .write()
+        .mode(SaveMode.Overwrite)
+        .format("text")
+        .save(metadataFileListPath);
+  }
+
+  private void saveDataFileList(Dataset<Row> dataFiles) {
+    dataFileListPath = stagingDir + DATA_FILE_LIST_DIR;
+
+    try {
+      dataFiles
+          .repartition(1)
+          .write()
+          .mode(SaveMode.Overwrite)
+          .format("text")
+          .save(dataFileListPath);
+    } catch (Exception e) {
+      throw new UnsupportedOperationException(
+          "Failed to build the data files dataframe, the end version you are "
+              + "trying to copy may contain invalid snapshots, please use the younger version which doesn't have invalid "
+              + "snapshots",
+          e);
+    }
+  }
+
+  private Set<Long> getDiffSnapshotIds(Set<Long> allSnapshotIds) {
+    Set<Long> snapshotIdsInStartVersion = Sets.newHashSet();
+    if (startStaticTable != null) {
+      startStaticTable
+          .snapshots()
+          .forEach(snapshot -> snapshotIdsInStartVersion.add(snapshot.snapshotId()));
+    }
+    return Sets.difference(allSnapshotIds, snapshotIdsInStartVersion);
+  }
+
+  private Set<Long> rewriteVersionFiles(TableMetadata metadata) {
+    Set<Long> allSnapshotIds = Sets.newHashSet();
+
+    String stagingPath = stagingPath(endVersion, stagingDir);
+    metadata.snapshots().forEach(snapshot -> allSnapshotIds.add(snapshot.snapshotId()));
+    rewriteVersionFile(metadata, stagingPath);
+
+    List<MetadataLogEntry> versions = metadata.previousFiles();
+    for (int i = versions.size() - 1; i >= 0; i--) {
+      String versionFilePath = versions.get(i).file();
+      if (versionFilePath.equals(startVersion)) {
+        break;
+      }
+
+      Preconditions.checkArgument(
+          fileExist(versionFilePath),
+          String.format("Version file %s doesn't exist", versionFilePath));
+      String newPath = stagingPath(versionFilePath, stagingDir);
+      TableMetadata tableMetadata =
+          new StaticTableOperations(versionFilePath, table.io()).current();
+
+      tableMetadata.snapshots().forEach(snapshot -> allSnapshotIds.add(snapshot.snapshotId()));
+
+      rewriteVersionFile(tableMetadata, newPath);
+    }
+
+    return allSnapshotIds;
+  }
+
+  private Set<Snapshot> snapshotSet(String metadataPath) {
+    Set<Snapshot> snapshots = Sets.newHashSet();
+    if (!metadataPath.isEmpty()) {
+      StaticTableOperations ops = new StaticTableOperations(metadataPath, table.io());
+      TableMetadata metadata = ops.current();
+      snapshots.addAll(metadata.snapshots());
+    }
+    return snapshots;
+  }
+
+  private void rewriteVersionFile(TableMetadata metadata, String stagingPath) {
+    TableMetadata newTableMetadata =
+        TableMetadataUtil.replacePaths(metadata, sourcePrefix, targetPrefix);
+    TableMetadataParser.overwrite(newTableMetadata, table.io().newOutputFile(stagingPath));
+    addToRebuiltFiles(stagingPath);
+  }
+
+  private void rewriteManifestList(Snapshot snapshot, TableMetadata tableMetadata) {
+    List<ManifestFile> manifestFiles = manifestFilesInSnapshot(snapshot);
+    String path = snapshot.manifestListLocation();
+    String stagingPath = stagingPath(path, stagingDir);
+    OutputFile outputFile = table.io().newOutputFile(stagingPath);
+    try (FileAppender<ManifestFile> writer =
+        ManifestLists.write(
+            tableMetadata.formatVersion(),
+            outputFile,
+            snapshot.snapshotId(),
+            snapshot.parentId(),
+            snapshot.sequenceNumber())) {
+
+      for (ManifestFile file : manifestFiles) {
+        // need to get the ManifestFile object for manifest file rewriting
+        if (manifestFilePaths.contains(file.path())) {
+          manifestFilesToRewrite.add(file);
+        }
+
+        ManifestFile newFile = file.copy();
+        if (newFile.path().startsWith(sourcePrefix)) {
+          ((StructLike) newFile).set(0, newPath(newFile.path(), sourcePrefix, targetPrefix));
+        }
+        writer.add(newFile);
+      }
+
+      addToRebuiltFiles(stagingPath);
+    } catch (IOException e) {
+      throw new UncheckedIOException("Failed to rewrite the manifest list file " + path, e);
+    }
+  }
+
+  private List<ManifestFile> manifestFilesInSnapshot(Snapshot snapshot) {
+    String path = snapshot.manifestListLocation();
+    List<ManifestFile> manifestFiles = Lists.newLinkedList();
+    try {
+      manifestFiles = ManifestLists.read(table.io().newInputFile(path));
+    } catch (RuntimeIOException e) {
+      LOG.warn("Failed to read manifest list {}", path, e);
+    }
+    return manifestFiles;
+  }
+
+  private List<String> manifestFilesToMove(Set<Long> diffSnapshotIds) {
+    try {
+      Dataset<Row> lastVersionFiles = buildManifestFileDF(endStaticTable);
+      if (startStaticTable == null) {
+        return lastVersionFiles.distinct().as(Encoders.STRING()).collectAsList();
+      } else {
+        return lastVersionFiles
+            .distinct()
+            .filter(functions.column("added_snapshot_id").isInCollection(diffSnapshotIds))
+            .as(Encoders.STRING())
+            .collectAsList();
+      }
+    } catch (Exception e) {
+      throw new UnsupportedOperationException(
+          "Failed to build the manifest files dataframe, the end version you are "
+              + "trying to copy may contain invalid snapshots, please use the younger version which doesn't have invalid "
+              + "snapshots",
+          e);
+    }
+  }
+
+  /** Rewrite manifest files in a distributed manner. */
+  private List<ManifestFile> rewriteManifests(TableMetadata tableMetadata) {
+    if (manifestFilesToRewrite.isEmpty()) {
+      return Lists.newArrayList();
+    }
+
+    Encoder<ManifestFile> manifestFileEncoder = Encoders.javaSerialization(ManifestFile.class);
+    Dataset<ManifestFile> manifestDS =
+        spark().createDataset(Lists.newArrayList(manifestFilesToRewrite), manifestFileEncoder);
+
+    FileIO tableIO = SerializableTable.copyOf(table).io();
+    Broadcast<FileIO> io = sparkContext().broadcast(tableIO);
+
+    return manifestDS
+        .repartition(manifestFilesToRewrite.size())
+        .mapPartitions(
+            toManifests(
+                io,
+                stagingDir,
+                tableMetadata.formatVersion(),
+                tableMetadata.specsById(),
+                sourcePrefix,
+                targetPrefix),
+            manifestFileEncoder)
+        .collectAsList();
+  }
+
+  private static MapPartitionsFunction<ManifestFile, ManifestFile> toManifests(
+      Broadcast<FileIO> io,
+      String stagingLocation,
+      int format,
+      Map<Integer, PartitionSpec> specsById,
+      String sourcePrefix,
+      String targetPrefix) {
+
+    return rows -> {
+      List<ManifestFile> manifests = Lists.newArrayList();
+      while (rows.hasNext()) {
+        manifests.add(
+            writeManifest(
+                rows.next(), io, stagingLocation, format, specsById, sourcePrefix, targetPrefix));
+      }
+
+      return manifests.iterator();
+    };
+  }
+
+  private static ManifestFile writeManifest(
+      ManifestFile manifestFile,
+      Broadcast<FileIO> io,
+      String stagingLocation,
+      int format,
+      Map<Integer, PartitionSpec> specsById,
+      String sourcePrefix,
+      String targetPrefix)
+      throws IOException {
+
+    String stagingPath = stagingPath(manifestFile.path(), stagingLocation);
+    OutputFile outputFile = io.value().newOutputFile(stagingPath);
+    PartitionSpec spec = specsById.get(manifestFile.partitionSpecId());
+    ManifestWriter<DataFile> writer =
+        ManifestFiles.write(format, spec, outputFile, manifestFile.snapshotId());
+
+    try (ManifestReader<DataFile> reader =
+        ManifestFiles.read(manifestFile, io.getValue(), specsById).select(Arrays.asList("*"))) {
+      reader
+          .entries()
+          .forEach(entry -> appendEntry(entry, writer, spec, sourcePrefix, targetPrefix));
+    } finally {
+      writer.close();
+    }
+
+    return writer.toManifestFile();
+  }
+
+  private static void appendEntry(
+      ManifestEntry<DataFile> entry,
+      ManifestWriter<DataFile> writer,
+      PartitionSpec spec,
+      String sourcePrefix,
+      String targetPrefix) {
+    DataFile dataFile = entry.file();
+    String dataFilePath = dataFile.path().toString();
+    if (dataFilePath.startsWith(sourcePrefix)) {
+      dataFilePath = newPath(dataFilePath, sourcePrefix, targetPrefix);
+      dataFile = DataFiles.builder(spec).copy(entry.file()).withPath(dataFilePath).build();
+    }
+
+    switch (entry.status()) {
+      case ADDED:
+        writer.add(dataFile);
+        break;
+      case EXISTING:
+        writer.existing(
+            dataFile, entry.snapshotId(), entry.dataSequenceNumber(), entry.fileSequenceNumber());
+        break;
+      case DELETED:
+        writer.delete(dataFile, entry.dataSequenceNumber(), entry.fileSequenceNumber());
+        break;
+    }
+  }
+
+  private Dataset<Row> getDiffDataFiles(Set<Long> diffSnapshotIds) {
+    Dataset<Row> lastVersionFiles = buildValidDataFileDFWithSnapshotId(endStaticTable);
+    if (startStaticTable == null) {
+      return lastVersionFiles.distinct().select("file_path");
+    } else {
+      return lastVersionFiles
+          .distinct()
+          .filter(functions.column("snapshot_id").isInCollection(diffSnapshotIds))
+          .select("file_path");
+    }
+  }
+
+  private boolean fileNotExist(String path) {
+    return !fileExist(path);
+  }
+
+  private boolean fileExist(String path) {
+    if (path == null || path.trim().isEmpty()) {
+      return false;
+    }
+    return table.io().newInputFile(path).exists();
+  }
+
+  private static String newPath(String path, String sourcePrefix, String targetPrefix) {
+    return path.replaceFirst(sourcePrefix, targetPrefix);
+  }
+
+  private void addToRebuiltFiles(String path) {
+    metadataFilesToMove.add(path);
+  }
+
+  private static String stagingPath(String originalPath, String stagingLocation) {
+    return stagingLocation + fileName(originalPath);
+  }
+
+  private String currentMetadataPath(Table tbl) {
+    return ((HasTableOperations) tbl).operations().current().metadataFileLocation();
+  }
+
+  private static String fileName(String path) {
+    String filename = path;
+    int lastIndex = path.lastIndexOf(File.separator);
+    if (lastIndex != -1) {
+      filename = path.substring(lastIndex + 1);
+    }
+    return filename;
+  }
+
+  private String getMetadataLocation(Table tbl) {
+    String currentMetadataPath =
+        ((HasTableOperations) tbl).operations().current().metadataFileLocation();
+    int lastIndex = currentMetadataPath.lastIndexOf(File.separator);
+    String metadataDir = "";
+    if (lastIndex != -1) {
+      metadataDir = currentMetadataPath.substring(0, lastIndex + 1);
+    }
+
+    Preconditions.checkArgument(
+        !metadataDir.isEmpty(), "Failed to get the metadata file root directory");
+    return metadataDir;
+  }
+}
diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseRemoveExpiredFilesSparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseRemoveExpiredFilesSparkAction.java
new file mode 100644
index 000000000..bf0099924
--- /dev/null
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseRemoveExpiredFilesSparkAction.java
@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.spark.actions;
+
+import java.io.File;
+import java.util.Iterator;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.function.Consumer;
+import org.apache.iceberg.HasTableOperations;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableMetadata;
+import org.apache.iceberg.actions.BaseRemoveExpiredFilesActionResult;
+import org.apache.iceberg.actions.RemoveExpiredFiles;
+import org.apache.iceberg.exceptions.NotFoundException;
+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
+import org.apache.iceberg.spark.JobGroupInfo;
+import org.apache.iceberg.util.Tasks;
+import org.apache.spark.sql.Column;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.SparkSession;
+import org.apache.spark.sql.functions;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class BaseRemoveExpiredFilesSparkAction extends BaseSparkAction<RemoveExpiredFiles>
+    implements RemoveExpiredFiles {
+
+  private static final Logger LOG =
+      LoggerFactory.getLogger(BaseRemoveExpiredFilesSparkAction.class);
+  private static final ExecutorService DEFAULT_EXECUTOR_SERVICE = null;
+  private static final String DATA_FILE = "Data File";
+  private static final String MANIFEST = "Manifest";
+  private static final String MANIFEST_LIST = "Manifest List";
+
+  private final Table table;
+  private ExecutorService executorService = DEFAULT_EXECUTOR_SERVICE;
+  private String targetVersion;
+  private Table targetTable;
+
+  private AtomicLong dataFileCount = new AtomicLong(0L);
+  private AtomicLong manifestCount = new AtomicLong(0L);
+  private AtomicLong manifestListCount = new AtomicLong(0L);
+
+  private final Consumer<String> deleteFunc =
+      new Consumer<String>() {
+        @Override
+        public void accept(String file) {
+          table.io().deleteFile(file);
+        }
+      };
+
+  public BaseRemoveExpiredFilesSparkAction(SparkSession spark, Table table) {
+    super(spark);
+    this.table = table;
+  }
+
+  @Override
+  protected RemoveExpiredFiles self() {
+    return this;
+  }
+
+  @Override
+  public RemoveExpiredFiles executeWith(ExecutorService service) {
+    this.executorService = service;
+    return this;
+  }
+
+  @Override
+  public RemoveExpiredFiles targetVersion(String tVersion) {
+    Preconditions.checkArgument(
+        tVersion != null && !tVersion.isEmpty(),
+        "Target version file('%s') cannot be empty.",
+        tVersion);
+
+    String tVersionFile = tVersion;
+    if (!tVersionFile.contains(File.separator)) {
+      tVersionFile = ((HasTableOperations) table).operations().metadataFileLocation(tVersionFile);
+    }
+
+    Preconditions.checkArgument(
+        fileExist(tVersionFile), "Version file('%s') doesn't exist.", tVersionFile);
+    this.targetVersion = tVersionFile;
+    return this;
+  }
+
+  @Override
+  public RemoveExpiredFiles.Result execute() {
+    JobGroupInfo info = newJobGroupInfo("REMOVE-EXPIRED-FILES", jobDesc());
+    return withJobGroupInfo(info, this::doExecute);
+  }
+
+  private String jobDesc() {
+    return String.format(
+        "Remove expired files in version '%s' of table %s.", targetVersion, table.name());
+  }
+
+  private RemoveExpiredFiles.Result doExecute() {
+    targetTable = newStaticTable(targetVersion, table.io());
+
+    deleteFiles(expiredFiles().collectAsList().iterator());
+
+    return new BaseRemoveExpiredFilesActionResult(
+        dataFileCount.get(), manifestCount.get(), manifestListCount.get());
+  }
+
+  private Dataset<Row> expiredFiles() {
+    Dataset<Row> originalFiles = buildValidFileDF(currentMetadata(table));
+    Dataset<Row> validFiles = buildValidFileDF(currentMetadata(targetTable));
+    return originalFiles.except(validFiles);
+  }
+
+  private TableMetadata currentMetadata(Table tbl) {
+    return ((HasTableOperations) tbl).operations().current();
+  }
+
+  private Dataset<Row> buildValidFileDF(TableMetadata metadata) {
+    Table staticTable = newStaticTable(metadata, this.table.io());
+    return appendTypeString(buildValidDataFileDF(staticTable), DATA_FILE)
+        .union(appendTypeString(buildManifestFileDF(staticTable), MANIFEST))
+        .union(appendTypeString(buildManifestListDF(staticTable), MANIFEST_LIST));
+  }
+
+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {
+    return ds.select(new Column("file_path"), functions.lit(type).as("file_type"));
+  }
+
+  private void deleteFiles(Iterator<Row> filesToDelete) {
+    Tasks.foreach(filesToDelete)
+        .retry(3)
+        .stopRetryOn(NotFoundException.class)
+        .suppressFailureWhenFinished()
+        .executeWith(executorService)
+        .onFailure(
+            (fileInfo, exc) -> {
+              String file = fileInfo.getString(0);
+              String type = fileInfo.getString(1);
+              LOG.warn("Delete failed for {}: {}", type, file, exc);
+            })
+        .run(
+            fileInfo -> {
+              String file = fileInfo.getString(0);
+              String type = fileInfo.getString(1);
+              deleteFunc.accept(file);
+              switch (type) {
+                case DATA_FILE:
+                  dataFileCount.incrementAndGet();
+                  LOG.trace("Deleted Data File: {}", file);
+                  break;
+                case MANIFEST:
+                  manifestCount.incrementAndGet();
+                  LOG.debug("Deleted Manifest: {}", file);
+                  break;
+                case MANIFEST_LIST:
+                  manifestListCount.incrementAndGet();
+                  LOG.debug("Deleted Manifest List: {}", file);
+                  break;
+              }
+            });
+
+    LOG.info(
+        "Deleted {} total files",
+        dataFileCount.get() + manifestCount.get() + manifestListCount.get());
+  }
+
+  private boolean fileExist(String path) {
+    if (path == null || path.trim().isEmpty()) {
+      return false;
+    }
+    return table.io().newInputFile(path).exists();
+  }
+}
diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java
index 3c007c621..0a7f1017b 100644
--- a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseSparkAction.java
@@ -33,6 +33,7 @@ import java.util.concurrent.atomic.AtomicLong;
 import java.util.function.Consumer;
 import java.util.function.Predicate;
 import java.util.function.Supplier;
+import java.util.stream.Stream;
 import org.apache.iceberg.AllManifestsTable;
 import org.apache.iceberg.BaseTable;
 import org.apache.iceberg.ContentFile;
@@ -43,10 +44,13 @@ import org.apache.iceberg.ManifestFiles;
 import org.apache.iceberg.MetadataTableType;
 import org.apache.iceberg.PartitionSpec;
 import org.apache.iceberg.ReachableFileUtil;
+import org.apache.iceberg.SerializableTable;
+import org.apache.iceberg.Snapshot;
 import org.apache.iceberg.StaticTableOperations;
 import org.apache.iceberg.StatisticsFile;
 import org.apache.iceberg.Table;
 import org.apache.iceberg.TableMetadata;
+import org.apache.iceberg.TableOperations;
 import org.apache.iceberg.exceptions.NotFoundException;
 import org.apache.iceberg.exceptions.ValidationException;
 import org.apache.iceberg.io.BulkDeletionFailureException;
@@ -62,10 +66,12 @@ import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.relocated.com.google.common.collect.Maps;
 import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;
+import org.apache.iceberg.relocated.com.google.common.collect.Streams;
 import org.apache.iceberg.spark.JobGroupInfo;
 import org.apache.iceberg.spark.JobGroupUtils;
 import org.apache.iceberg.spark.SparkTableUtil;
 import org.apache.iceberg.spark.source.SerializableTableWithSize;
+import org.apache.iceberg.util.Pair;
 import org.apache.iceberg.util.Tasks;
 import org.apache.spark.SparkContext;
 import org.apache.spark.api.java.JavaSparkContext;
@@ -73,10 +79,12 @@ import org.apache.spark.api.java.function.FlatMapFunction;
 import org.apache.spark.broadcast.Broadcast;
 import org.apache.spark.sql.Column;
 import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Encoders;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.SparkSession;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import scala.Tuple2;
 
 abstract class BaseSparkAction<ThisT> {
 
@@ -129,6 +137,68 @@ abstract class BaseSparkAction<ThisT> {
     return options;
   }
 
+  /**
+   * Returns all the path locations of all Manifest Lists for a given list of snapshots
+   *
+   * @param snapshots snapshots
+   * @return the paths of the Manifest Lists
+   */
+  private List<String> getManifestListPaths(Iterable<Snapshot> snapshots) {
+    List<String> manifestLists = Lists.newArrayList();
+    for (Snapshot snapshot : snapshots) {
+      String manifestListLocation = snapshot.manifestListLocation();
+      if (manifestListLocation != null) {
+        manifestLists.add(manifestListLocation);
+      }
+    }
+    return manifestLists;
+  }
+
+  /**
+   * Returns all Metadata file paths which may not be in the current metadata. Specifically this
+   * includes "version-hint" files as well as entries in metadata.previousFiles.
+   *
+   * @param ops TableOperations for the table we will be getting paths from
+   * @return a list of paths to metadata files
+   */
+  private List<String> getOtherMetadataFilePaths(TableOperations ops) {
+    List<String> otherMetadataFiles = Lists.newArrayList();
+    otherMetadataFiles.add(ops.metadataFileLocation("version-hint.text"));
+
+    TableMetadata metadata = ops.current();
+    otherMetadataFiles.add(metadata.metadataFileLocation());
+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {
+      otherMetadataFiles.add(previousMetadataFile.file());
+    }
+    return otherMetadataFiles;
+  }
+
+  protected Dataset<Row> buildOtherMetadataFileDF(Table table) {
+    return buildOtherMetadataFileDF(
+        table, false /* include all reachable previous metadata locations */);
+  }
+
+  protected Dataset<Row> buildAllReachableOtherMetadataFileDF(Table table) {
+    return buildOtherMetadataFileDF(
+        table, true /* include all reachable previous metadata locations */);
+  }
+
+  private Dataset<Row> buildOtherMetadataFileDF(
+      Table table, boolean includePreviousMetadataLocations) {
+    List<String> otherMetadataFiles = Lists.newArrayList();
+    otherMetadataFiles.addAll(
+        ReachableFileUtil.metadataFileLocations(table, includePreviousMetadataLocations));
+    otherMetadataFiles.add(ReachableFileUtil.versionHintLocation(table));
+    return spark.createDataset(otherMetadataFiles, Encoders.STRING()).toDF(FILE_PATH);
+  }
+
+  protected Dataset<Row> buildValidMetadataFileDF(Table table) {
+    Dataset<Row> manifestDF = buildManifestFileDF(table);
+    Dataset<Row> manifestListDF = buildManifestListDF(table);
+    Dataset<Row> otherMetadataFileDF = buildOtherMetadataFileDF(table);
+    return manifestDF.union(otherMetadataFileDF).union(manifestListDF);
+  }
+
   protected <T> T withJobGroupInfo(JobGroupInfo info, Supplier<T> supplier) {
     SparkContext context = spark().sparkContext();
     JobGroupInfo previousInfo = JobGroupUtils.getJobGroupInfo(context);
@@ -145,7 +215,10 @@ abstract class BaseSparkAction<ThisT> {
   }
 
   protected Table newStaticTable(TableMetadata metadata, FileIO io) {
-    String metadataFileLocation = metadata.metadataFileLocation();
+    return newStaticTable(metadata.metadataFileLocation(), io);
+  }
+
+  protected Table newStaticTable(String metadataFileLocation, FileIO io) {
     StaticTableOperations ops = new StaticTableOperations(metadataFileLocation, io);
     return new BaseTable(ops, metadataFileLocation);
   }
@@ -184,6 +257,68 @@ abstract class BaseSparkAction<ThisT> {
         .as(FileInfo.ENCODER);
   }
 
+  protected Dataset<Row> buildValidDataFileDF(Table table) {
+    JavaSparkContext context = JavaSparkContext.fromSparkContext(spark.sparkContext());
+    Broadcast<Table> tableBroadcast = context.broadcast(SerializableTable.copyOf(table));
+
+    Dataset<ManifestFileBean> allManifests =
+        loadMetadataTable(table, ALL_MANIFESTS)
+            .selectExpr(
+                "content",
+                "path",
+                "length",
+                "partition_spec_id as partitionSpecId",
+                "added_snapshot_id as addedSnapshotId")
+            .dropDuplicates("path")
+            .repartition(
+                spark
+                    .sessionState()
+                    .conf()
+                    .numShufflePartitions()) // avoid adaptive execution combining tasks
+            .as(Encoders.bean(ManifestFileBean.class));
+
+    return allManifests
+        .flatMap(new ReadManifest(tableBroadcast), FileInfo.ENCODER)
+        .toDF("file_path", "file_type")
+        .drop("file_type");
+  }
+
+  protected Dataset<Row> buildValidDataFileDFWithSnapshotId(Table table) {
+    JavaSparkContext context = JavaSparkContext.fromSparkContext(spark.sparkContext());
+    Broadcast<FileIO> ioBroadcast = context.broadcast(SerializableTable.copyOf(table).io());
+
+    Dataset<ManifestFileBean> allManifests =
+        loadMetadataTable(table, ALL_MANIFESTS)
+            .selectExpr(
+                "content",
+                "path",
+                "length",
+                "partition_spec_id as partitionSpecId",
+                "added_snapshot_id as addedSnapshotId")
+            .dropDuplicates("path")
+            .repartition(
+                spark
+                    .sessionState()
+                    .conf()
+                    .numShufflePartitions()) // avoid adaptive execution combining tasks
+            .as(Encoders.bean(ManifestFileBean.class));
+
+    return allManifests
+        .flatMap(
+            new ReadManifestWithSnapshotId(ioBroadcast),
+            Encoders.tuple(Encoders.STRING(), Encoders.LONG()))
+        .toDF("file_path", "snapshot_id");
+  }
+
+  protected Dataset<Row> buildManifestFileDF(Table table) {
+    return loadMetadataTable(table, ALL_MANIFESTS).select(col("path").as(FILE_PATH));
+  }
+
+  protected Dataset<Row> buildManifestListDF(Table table) {
+    List<String> manifestLists = getManifestListPaths(table.snapshots());
+    return spark.createDataset(manifestLists, Encoders.STRING()).toDF("file_path");
+  }
+
   private Dataset<Row> manifestDF(Table table, Set<Long> snapshotIds) {
     Dataset<Row> manifestDF = loadMetadataTable(table, ALL_MANIFESTS);
     if (snapshotIds != null) {
@@ -451,4 +586,22 @@ abstract class BaseSparkAction<ThisT> {
       return new FileInfo(file.path().toString(), file.content().toString());
     }
   }
+
+  private static class ReadManifestWithSnapshotId
+      implements FlatMapFunction<ManifestFileBean, Tuple2<String, Long>> {
+    private final Broadcast<FileIO> io;
+
+    ReadManifestWithSnapshotId(Broadcast<FileIO> io) {
+      this.io = io;
+    }
+
+    @Override
+    public Iterator<Tuple2<String, Long>> call(ManifestFileBean manifest) {
+      Iterator<Pair<String, Long>> iterator =
+          new ClosingIterator<>(
+              ManifestFiles.readPathsWithSnapshotId(manifest, io.getValue()).iterator());
+      Stream<Pair<String, Long>> stream = Streams.stream(iterator);
+      return stream.map(pair -> Tuple2.apply(pair.first(), pair.second())).iterator();
+    }
+  }
 }
diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java
index fb67ded96..7f3b80b75 100644
--- a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/SparkActions.java
@@ -20,6 +20,9 @@ package org.apache.iceberg.spark.actions;
 
 import org.apache.iceberg.Table;
 import org.apache.iceberg.actions.ActionsProvider;
+import org.apache.iceberg.actions.CheckSnapshotIntegrity;
+import org.apache.iceberg.actions.CopyTable;
+import org.apache.iceberg.actions.RemoveExpiredFiles;
 import org.apache.iceberg.spark.Spark3Util;
 import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;
 import org.apache.spark.sql.SparkSession;
@@ -96,4 +99,19 @@ public class SparkActions implements ActionsProvider {
   public RewritePositionDeleteFilesSparkAction rewritePositionDeletes(Table table) {
     return new RewritePositionDeleteFilesSparkAction(spark, table);
   }
+
+  @Override
+  public CopyTable copyTable(Table table) {
+    return new BaseCopyTableSparkAction(spark, table);
+  }
+
+  @Override
+  public CheckSnapshotIntegrity checkSnapshotIntegrity(Table table) {
+    return new BaseCheckSnapshotIntegritySparkAction(spark, table);
+  }
+
+  @Override
+  public RemoveExpiredFiles removeExpiredFiles(Table table) {
+    return new BaseRemoveExpiredFilesSparkAction(spark, table);
+  }
 }
diff --git a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCheckSnapshotIntegrityAction.java b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCheckSnapshotIntegrityAction.java
new file mode 100644
index 000000000..349525741
--- /dev/null
+++ b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCheckSnapshotIntegrityAction.java
@@ -0,0 +1,207 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.spark.actions;
+
+import static org.apache.iceberg.types.Types.NestedField.optional;
+
+import java.io.File;
+import java.util.List;
+import java.util.Set;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.AssertHelpers;
+import org.apache.iceberg.BaseTable;
+import org.apache.iceberg.HasTableOperations;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.StaticTableOperations;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableMetadata;
+import org.apache.iceberg.actions.ActionsProvider;
+import org.apache.iceberg.actions.CheckSnapshotIntegrity;
+import org.apache.iceberg.hadoop.HadoopTables;
+import org.apache.iceberg.io.FileIO;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
+import org.apache.iceberg.spark.SparkTestBase;
+import org.apache.iceberg.spark.source.ThreeColumnRecord;
+import org.apache.iceberg.types.Types;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Encoders;
+import org.apache.spark.sql.Row;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+public class TestCheckSnapshotIntegrityAction extends SparkTestBase {
+  private ActionsProvider actions() {
+    return SparkActions.get();
+  }
+
+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());
+  protected static final Schema SCHEMA =
+      new Schema(
+          optional(1, "c1", Types.IntegerType.get()),
+          optional(2, "c2", Types.StringType.get()),
+          optional(3, "c3", Types.StringType.get()));
+
+  @Rule public TemporaryFolder temp = new TemporaryFolder();
+  private File tableDir = null;
+  protected String tableLocation = null;
+  private Table table = null;
+
+  @Before
+  public void setupTableLocation() throws Exception {
+    this.tableDir = temp.newFolder();
+    this.tableLocation = tableDir.toURI().toString();
+    this.table = createTableWith2Snapshots();
+  }
+
+  @Test
+  public void testCheckSnapshotIntegrity() {
+    List<String> validFiles = validFiles();
+
+    String metadataPath =
+        ((HasTableOperations) table).operations().current().metadataFileLocation();
+    String startVersionFile = tableLocation + "metadata/v1.metadata.json";
+    Table startTable = newStaticTable(startVersionFile, table.io());
+    CheckSnapshotIntegrity.Result result =
+        actions().checkSnapshotIntegrity(startTable).targetVersion(metadataPath).execute();
+    checkMissingFiles(0, result);
+
+    // delete one file
+    table.io().deleteFile(validFiles.get(0));
+    CheckSnapshotIntegrity.Result result1 =
+        actions().checkSnapshotIntegrity(startTable).targetVersion(metadataPath).execute();
+    checkMissingFiles(1, result1);
+
+    // delete another file
+    table.io().deleteFile(validFiles.get(1));
+    CheckSnapshotIntegrity.Result result2 =
+        actions().checkSnapshotIntegrity(startTable).targetVersion(metadataPath).execute();
+    checkMissingFiles(2, result2);
+  }
+
+  @Test
+  public void testStartFromFirstSnapshot() {
+    List<String> validFiles = validFiles();
+
+    String metadataPath =
+        ((HasTableOperations) table).operations().current().metadataFileLocation();
+
+    String startVersionFile = tableLocation + "metadata/v2.metadata.json";
+    Table startTable = newStaticTable(startVersionFile, table.io());
+    CheckSnapshotIntegrity.Result result =
+        actions().checkSnapshotIntegrity(startTable).targetVersion(metadataPath).execute();
+    checkMissingFiles(0, result);
+
+    // delete the file added by the first snapshot
+    table.io().deleteFile(validFiles.get(1));
+    CheckSnapshotIntegrity.Result result1 =
+        actions().checkSnapshotIntegrity(startTable).targetVersion(metadataPath).execute();
+    checkMissingFiles(0, result1);
+
+    // delete the file added by the first snapshot
+    table.io().deleteFile(validFiles.get(0));
+    CheckSnapshotIntegrity.Result result2 =
+        actions().checkSnapshotIntegrity(startTable).targetVersion(metadataPath).execute();
+    checkMissingFiles(1, result2);
+  }
+
+  @Test
+  public void testInputs() {
+    CheckSnapshotIntegrity actions = actions().checkSnapshotIntegrity(table);
+
+    AssertHelpers.assertThrows("", IllegalArgumentException.class, () -> actions.targetVersion(""));
+    AssertHelpers.assertThrows(
+        "", IllegalArgumentException.class, () -> actions.targetVersion(null));
+    AssertHelpers.assertThrows(
+        "", IllegalArgumentException.class, () -> actions.targetVersion("invalid"));
+
+    // either version file name or path are valid
+    String versionFilePath = currentMetadata(table).metadataFileLocation();
+    actions.targetVersion(versionFilePath);
+    String versionFilename = fileName(versionFilePath);
+    actions.targetVersion(versionFilename);
+  }
+
+  private void checkMissingFiles(int num, CheckSnapshotIntegrity.Result result) {
+    Assert.assertEquals(
+        "Missing file count should be", num, ((Set) result.missingFileLocations()).size());
+  }
+
+  private Table newStaticTable(String metadataFileLocation, FileIO io) {
+    StaticTableOperations ops = new StaticTableOperations(metadataFileLocation, io);
+    return new BaseTable(ops, metadataFileLocation);
+  }
+
+  private List<String> validFiles() {
+    return validFiles(tableLocation);
+  }
+
+  private List<String> validFiles(String location) {
+    return spark
+        .read()
+        .format("iceberg")
+        .load(location + "#files")
+        .select("file_path")
+        .as(Encoders.STRING())
+        .collectAsList();
+  }
+
+  private Table createTableWith2Snapshots() {
+    return createTableWith2Snapshots(tableLocation);
+  }
+
+  private Table createTableWith2Snapshots(String tblLocation) {
+    return createTableWithSnapshots(tblLocation, 2);
+  }
+
+  private Table createTableWithSnapshots(String tblLocation, int snapshotNumber) {
+    Table tbl =
+        TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tblLocation);
+
+    List<ThreeColumnRecord> records =
+        Lists.newArrayList(new ThreeColumnRecord(1, "AAAAAAAAAA", "AAAA"));
+
+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);
+
+    for (int i = 0; i < snapshotNumber; i++) {
+      df.select("c1", "c2", "c3").write().format("iceberg").mode("append").save(tblLocation);
+    }
+
+    tbl.refresh();
+
+    return tbl;
+  }
+
+  private TableMetadata currentMetadata(Table tbl) {
+    return ((HasTableOperations) tbl).operations().current();
+  }
+
+  private static String fileName(String path) {
+    String filename = path;
+    int lastIndex = path.lastIndexOf(File.separator);
+    if (lastIndex != -1) {
+      filename = path.substring(lastIndex + 1);
+    }
+    return filename;
+  }
+}
diff --git a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
new file mode 100644
index 000000000..7d44ff0e3
--- /dev/null
+++ b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
@@ -0,0 +1,918 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.spark.actions;
+
+import static org.apache.iceberg.types.Types.NestedField.optional;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import org.apache.commons.io.FileUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.AssertHelpers;
+import org.apache.iceberg.BaseTable;
+import org.apache.iceberg.HasTableOperations;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.StaticTableOperations;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableMetadata;
+import org.apache.iceberg.TableProperties;
+import org.apache.iceberg.actions.ActionsProvider;
+import org.apache.iceberg.actions.CopyTable;
+import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.hadoop.HadoopTables;
+import org.apache.iceberg.io.FileIO;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
+import org.apache.iceberg.spark.SparkCatalog;
+import org.apache.iceberg.spark.SparkTestBase;
+import org.apache.iceberg.spark.source.ThreeColumnRecord;
+import org.apache.iceberg.types.Types;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Encoders;
+import org.apache.spark.sql.Row;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+public class TestCopyTableAction extends SparkTestBase {
+  protected ActionsProvider actions() {
+    return SparkActions.get();
+  }
+
+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());
+  protected static final Schema SCHEMA =
+      new Schema(
+          optional(1, "c1", Types.IntegerType.get()),
+          optional(2, "c2", Types.StringType.get()),
+          optional(3, "c3", Types.StringType.get()));
+
+  @Rule public TemporaryFolder temp = new TemporaryFolder();
+  private File tableDir = null;
+  protected String tableLocation = null;
+  private Table table = null;
+
+  @Before
+  public void setupTableLocation() throws Exception {
+    this.tableDir = temp.newFolder();
+    this.tableLocation = tableDir.toURI().toString();
+    this.table = createATableWith2Snapshots(tableLocation);
+  }
+
+  private Table createATableWith2Snapshots(String location) {
+    return createTableWithSnapshots(location, 2);
+  }
+
+  private Table createTableWithSnapshots(String location, int snapshotNumber) {
+    return createTableWithSnapshots(location, snapshotNumber, Maps.newHashMap());
+  }
+
+  protected Table createTableWithSnapshots(
+      String location, int snapshotNumber, Map<String, String> properties) {
+    Table newTable = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, location);
+
+    List<ThreeColumnRecord> records =
+        Lists.newArrayList(new ThreeColumnRecord(1, "AAAAAAAAAA", "AAAA"));
+
+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);
+
+    for (int i = 0; i < snapshotNumber; i++) {
+      df.select("c1", "c2", "c3").write().format("iceberg").mode("append").save(location);
+    }
+
+    return newTable;
+  }
+
+  @Test
+  public void testCopyTable() throws Exception {
+    String targetTableLocation = newTableLocation();
+
+    // check the data file location before the rebuild
+    List<String> validDataFiles =
+        spark
+            .read()
+            .format("iceberg")
+            .load(tableLocation + "#files")
+            .select("file_path")
+            .as(Encoders.STRING())
+            .collectAsList();
+    Assert.assertEquals("Should be 2 valid data files", 2, validDataFiles.size());
+
+    CopyTable.Result result =
+        actions()
+            .copyTable(table)
+            .rewriteLocationPrefix(tableLocation, targetTableLocation)
+            .endVersion("v3.metadata.json")
+            .execute();
+
+    Assert.assertEquals("The latest version should be", "v3.metadata.json", result.latestVersion());
+
+    checkMetadataFileNum(3, 2, 2, result);
+    checkDataFileNum(2, result);
+
+    // copy the metadata files and data files
+    moveTableFiles(tableLocation, targetTableLocation, stagingDir(result));
+
+    // verify the data file path after the rebuild
+    List<String> validDataFilesAfterRebuilt =
+        spark
+            .read()
+            .format("iceberg")
+            .load(targetTableLocation + "#files")
+            .select("file_path")
+            .as(Encoders.STRING())
+            .collectAsList();
+    Assert.assertEquals("Should be 2 valid data files", 2, validDataFilesAfterRebuilt.size());
+    for (String item : validDataFilesAfterRebuilt) {
+      Assert.assertTrue(
+          "Data file should point to the new location", item.startsWith(targetTableLocation));
+    }
+
+    // verify data rows
+    Dataset<Row> resultDF = spark.read().format("iceberg").load(targetTableLocation);
+    List<ThreeColumnRecord> actualRecords =
+        resultDF.sort("c1", "c2", "c3").as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();
+
+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();
+    expectedRecords.add(new ThreeColumnRecord(1, "AAAAAAAAAA", "AAAA"));
+    expectedRecords.add(new ThreeColumnRecord(1, "AAAAAAAAAA", "AAAA"));
+
+    Assert.assertEquals("Rows must match", expectedRecords, actualRecords);
+  }
+
+  @Test
+  public void testDataFilesDiff() throws Exception {
+    CopyTable.Result result =
+        actions()
+            .copyTable(table)
+            .rewriteLocationPrefix(tableLocation, newTableLocation())
+            .lastCopiedVersion("v2.metadata.json")
+            .execute();
+
+    checkDataFileNum(1, result);
+
+    List<String> rebuiltFiles =
+        spark
+            .read()
+            .format("text")
+            .load(result.metadataFileListLocation())
+            .as(Encoders.STRING())
+            .collectAsList();
+
+    // v3.metadata.json, one manifest-list file, one manifest file
+    checkMetadataFileNum(3, result);
+
+    String currentSnapshotId = String.valueOf(table.currentSnapshot().snapshotId());
+    Assert.assertTrue(
+        "Should have the current snapshot file",
+        rebuiltFiles.stream().filter(c -> c.contains(currentSnapshotId)).count() == 1);
+
+    String parentSnapshotId = String.valueOf(table.currentSnapshot().parentId());
+    Assert.assertTrue(
+        "Should NOT have the parent snapshot file",
+        rebuiltFiles.stream().filter(c -> c.contains(parentSnapshotId)).count() == 0);
+  }
+
+  @Test
+  public void testTableWith3Snapshots() throws Exception {
+    String location = newTableLocation();
+    Table tableWith3Snaps = createTableWithSnapshots(location, 3);
+    CopyTable.Result result =
+        actions()
+            .copyTable(tableWith3Snaps)
+            .rewriteLocationPrefix(location, newTableLocation())
+            .lastCopiedVersion("v2.metadata.json")
+            .execute();
+
+    checkMetadataFileNum(2, 2, 2, result);
+    checkDataFileNum(2, result);
+
+    // start from the first version
+    CopyTable.Result result1 =
+        actions()
+            .copyTable(tableWith3Snaps)
+            .rewriteLocationPrefix(location, newTableLocation())
+            .lastCopiedVersion("v1.metadata.json")
+            .execute();
+
+    checkMetadataFileNum(3, 3, 3, result1);
+    checkDataFileNum(3, result1);
+  }
+
+  @Test
+  public void testFullTableCopy() throws Exception {
+    CopyTable.Result result =
+        actions()
+            .copyTable(table)
+            .rewriteLocationPrefix(tableLocation, newTableLocation())
+            .execute();
+
+    checkMetadataFileNum(3, 2, 2, result);
+    checkDataFileNum(2, result);
+  }
+
+  @Test
+  public void testDeleteDataFile() throws Exception {
+    String location = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(location);
+    List<String> validDataFiles =
+        spark
+            .read()
+            .format("iceberg")
+            .load(location + "#files")
+            .select("file_path")
+            .as(Encoders.STRING())
+            .collectAsList();
+
+    sourceTable.newDelete().deleteFile(validDataFiles.stream().findFirst().get()).commit();
+
+    String targetLocation = newTableLocation();
+    CopyTable.Result result =
+        actions().copyTable(sourceTable).rewriteLocationPrefix(location, targetLocation).execute();
+
+    checkMetadataFileNum(4, 3, 3, result);
+    checkDataFileNum(2, result);
+
+    // copy the metadata files and data files
+    moveTableFiles(location, targetLocation, stagingDir(result));
+
+    // verify data rows
+    Dataset<Row> resultDF = spark.read().format("iceberg").load(targetLocation);
+    Assert.assertEquals(
+        "There are only one row left since we deleted a data file",
+        1,
+        resultDF.as(Encoders.bean(ThreeColumnRecord.class)).count());
+  }
+
+  @Test
+  public void testFullTableCopyWithDeletedVersionFiles() throws Exception {
+    String location = newTableLocation();
+    Table sourceTable = createTableWithSnapshots(location, 2);
+    // expire the first snapshot
+    Table staticTable = newStaticTable(location + "metadata/v2.metadata.json", table.io());
+    actions()
+        .expireSnapshots(sourceTable)
+        .expireSnapshotId(staticTable.currentSnapshot().snapshotId())
+        .execute();
+
+    // create 100 more snapshots
+    List<ThreeColumnRecord> records =
+        Lists.newArrayList(new ThreeColumnRecord(1, "AAAAAAAAAA", "AAAA"));
+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);
+    for (int i = 0; i < 100; i++) {
+      df.select("c1", "c2", "c3").write().format("iceberg").mode("append").save(location);
+    }
+    sourceTable.refresh();
+
+    // v1/v2/v3.metadata.json has been deleted in v104.metadata.json, and there is no way to find
+    // the first snapshot
+    // from the version file history
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(location, newTableLocation())
+            .execute();
+
+    // you can only find 101 snapshots but the manifest file and data file count should be 102.
+    checkMetadataFileNum(101, 101, 101, result);
+    checkDataFileNum(102, result);
+  }
+
+  protected Table newStaticTable(String metadataFileLocation, FileIO io) {
+    StaticTableOperations ops = new StaticTableOperations(metadataFileLocation, io);
+    return new BaseTable(ops, metadataFileLocation);
+  }
+
+  @Test
+  public void testRewriteTableWithoutSnapshot() throws Exception {
+    CopyTable.Result result =
+        actions()
+            .copyTable(table)
+            .rewriteLocationPrefix(tableLocation, newTableLocation())
+            .endVersion("v1.metadata.json")
+            .execute();
+
+    // the only rebuilt file is v1.metadata.json since it contains no snapshot
+    checkMetadataFileNum(1, result);
+    checkDataFileNum(0, result);
+  }
+
+  @Test
+  public void testExpireSnapshotBeforeRewrite() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(sourceTableLocation);
+
+    // expire one snapshot
+    actions()
+        .expireSnapshots(sourceTable)
+        .expireSnapshotId(sourceTable.currentSnapshot().parentId())
+        .execute();
+
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .execute();
+
+    checkMetadataFileNum(4, 1, 2, result);
+
+    checkDataFileNum(2, result);
+  }
+
+  @Test
+  public void testStartSnapshotWithoutValidSnapshot() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(sourceTableLocation);
+
+    // expire one snapshot
+    actions()
+        .expireSnapshots(sourceTable)
+        .expireSnapshotId(sourceTable.currentSnapshot().parentId())
+        .execute();
+
+    Assert.assertEquals(
+        "1 out 2 snapshot has been removed", 1, ((List) sourceTable.snapshots()).size());
+
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .lastCopiedVersion("v2.metadata.json")
+            .execute();
+
+    // 2 metadata.json, 1 manifest list file, 1 manifest files
+    checkMetadataFileNum(4, result);
+    checkDataFileNum(1, result);
+  }
+
+  @Test
+  public void testMoveTheVersionExpireSnapshot() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(sourceTableLocation);
+
+    // expire one snapshot
+    actions()
+        .expireSnapshots(sourceTable)
+        .expireSnapshotId(sourceTable.currentSnapshot().parentId())
+        .execute();
+
+    // only move version v4, which is the version generated by snapshot expiration
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .lastCopiedVersion("v3.metadata.json")
+            .execute();
+
+    // only v4.metadata.json needs to move
+    checkMetadataFileNum(1, result);
+    // no data file needs to move
+    checkDataFileNum(0, result);
+  }
+
+  @Test
+  public void testMoveVersionWithInvalidSnapshots() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(sourceTableLocation);
+
+    // expire one snapshot
+    actions()
+        .expireSnapshots(sourceTable)
+        .expireSnapshotId(sourceTable.currentSnapshot().parentId())
+        .execute();
+
+    AssertHelpers.assertThrows(
+        "Copy a version with invalid snapshots aren't allowed",
+        UnsupportedOperationException.class,
+        () ->
+            actions()
+                .copyTable(sourceTable)
+                .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+                .endVersion("v3.metadata.json")
+                .execute());
+  }
+
+  @Test
+  public void testRollBack() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(sourceTableLocation);
+    Long secondSnapshotId = sourceTable.currentSnapshot().snapshotId();
+
+    // roll back to the first snapshot(v2)
+    sourceTable
+        .manageSnapshots()
+        .setCurrentSnapshot(sourceTable.currentSnapshot().parentId())
+        .commit();
+
+    // add a new snapshot
+    List<ThreeColumnRecord> records =
+        Lists.newArrayList(new ThreeColumnRecord(1, "AAAAAAAAAA", "AAAA"));
+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);
+    df.select("c1", "c2", "c3").write().format("iceberg").mode("append").save(sourceTableLocation);
+
+    sourceTable.refresh();
+
+    // roll back to the second snapshot(v3)
+    sourceTable.manageSnapshots().setCurrentSnapshot(secondSnapshotId).commit();
+    // copy table
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .execute();
+
+    // check the result
+    checkMetadataFileNum(6, 3, 3, result);
+  }
+
+  @Test
+  public void testWriteAuditPublish() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(sourceTableLocation);
+
+    // enable WAP
+    sourceTable
+        .updateProperties()
+        .set(TableProperties.WRITE_AUDIT_PUBLISH_ENABLED, "true")
+        .commit();
+    spark.conf().set("spark.wap.id", "1");
+
+    // add a new snapshot without changing the current snapshot of the table
+    List<ThreeColumnRecord> records =
+        Lists.newArrayList(new ThreeColumnRecord(1, "AAAAAAAAAA", "AAAA"));
+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);
+    df.select("c1", "c2", "c3").write().format("iceberg").mode("append").save(sourceTableLocation);
+
+    sourceTable.refresh();
+
+    // copy table
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .execute();
+
+    // check the result. There are 3 snapshots in total, although the current snapshot is the second
+    // one.
+    checkMetadataFileNum(5, 3, 3, result);
+  }
+
+  @Test
+  public void testSchemaChange() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(sourceTableLocation);
+
+    // change the schema
+    sourceTable.updateSchema().addColumn("c4", Types.StringType.get()).commit();
+
+    // copy table
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .execute();
+
+    // check the result
+    checkMetadataFileNum(4, 2, 2, result);
+  }
+
+  @Test
+  public void testWithTargetTable() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createTableWithSnapshots(sourceTableLocation, 3);
+    String targetTableLocation = newTableLocation();
+    Table targetTable = createATableWith2Snapshots(targetTableLocation);
+
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, targetTableLocation)
+            .targetTable(targetTable)
+            .execute();
+
+    Assert.assertEquals("The latest version should be", "v4.metadata.json", result.latestVersion());
+
+    // 3 files rebuilt from v3 to v4: v4.metadata.json, one manifest list, one manifest file
+    checkMetadataFileNum(3, result);
+  }
+
+  @Test
+  public void testInvalidStartVersion() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createTableWithSnapshots(sourceTableLocation, 3);
+    String targetTableLocation = newTableLocation();
+    Table targetTable = createATableWith2Snapshots(targetTableLocation);
+
+    AssertHelpers.assertThrows(
+        "The valid start version should be v3",
+        IllegalArgumentException.class,
+        "The start version isn't the current version of the target table.",
+        () ->
+            actions()
+                .copyTable(sourceTable)
+                .rewriteLocationPrefix(sourceTableLocation, targetTableLocation)
+                .targetTable(targetTable)
+                .lastCopiedVersion("v2.metadata.json")
+                .execute());
+  }
+
+  @Test
+  public void testSnapshotIdInheritanceEnabled() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Map<String, String> properties = Maps.newHashMap();
+    properties.put(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, "true");
+
+    Table sourceTable = createTableWithSnapshots(sourceTableLocation, 2, properties);
+
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .execute();
+
+    checkMetadataFileNum(7, result);
+    checkDataFileNum(2, result);
+  }
+
+  @Test
+  public void testMetadataCompression() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Map<String, String> properties = Maps.newHashMap();
+    properties.put(TableProperties.METADATA_COMPRESSION, "gzip");
+    Table sourceTable = createTableWithSnapshots(sourceTableLocation, 2, properties);
+
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .endVersion("v2.gz.metadata.json")
+            .execute();
+
+    checkMetadataFileNum(4, result);
+    checkDataFileNum(1, result);
+
+    result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .lastCopiedVersion("v1.gz.metadata.json")
+            .execute();
+
+    checkMetadataFileNum(6, result);
+    checkDataFileNum(2, result);
+  }
+
+  @Test
+  public void testInvalidArgs() {
+    CopyTable actions = actions().copyTable(table);
+
+    AssertHelpers.assertThrows(
+        "",
+        IllegalArgumentException.class,
+        "Source prefix('') cannot be empty",
+        () -> actions.rewriteLocationPrefix("", null));
+
+    AssertHelpers.assertThrows(
+        "",
+        IllegalArgumentException.class,
+        "Source prefix('null') cannot be empty",
+        () -> actions.rewriteLocationPrefix(null, null));
+
+    AssertHelpers.assertThrows(
+        "",
+        IllegalArgumentException.class,
+        "Staging location('') cannot be empty",
+        () -> actions.stagingLocation(""));
+
+    AssertHelpers.assertThrows(
+        "",
+        IllegalArgumentException.class,
+        "Staging location('null') cannot be empty",
+        () -> actions.stagingLocation(null));
+
+    AssertHelpers.assertThrows(
+        "",
+        IllegalArgumentException.class,
+        "Last copied version('null') cannot be empty",
+        () -> actions.lastCopiedVersion(null));
+
+    AssertHelpers.assertThrows(
+        "Last copied version cannot be empty",
+        IllegalArgumentException.class,
+        () -> actions.lastCopiedVersion(" "));
+
+    AssertHelpers.assertThrows(
+        "End version cannot be empty",
+        IllegalArgumentException.class,
+        () -> actions.endVersion(" "));
+
+    AssertHelpers.assertThrows(
+        "End version cannot be empty",
+        IllegalArgumentException.class,
+        () -> actions.endVersion(null));
+  }
+
+  protected void checkDataFileNum(long count, CopyTable.Result result) {
+    List<String> filesToMove =
+        spark
+            .read()
+            .format("text")
+            .load(result.dataFileListLocation())
+            .as(Encoders.STRING())
+            .collectAsList();
+    Assert.assertEquals("The rebuilt data file number should be", count, filesToMove.size());
+  }
+
+  protected void checkMetadataFileNum(int count, CopyTable.Result result) {
+    List<String> filesToMove =
+        spark
+            .read()
+            .format("text")
+            .load(result.metadataFileListLocation())
+            .as(Encoders.STRING())
+            .collectAsList();
+    Assert.assertEquals("The rebuilt metadata file number should be", count, filesToMove.size());
+  }
+
+  protected void checkMetadataFileNum(
+      int versionFileCount, int manifestListCount, int manifestFileCount, CopyTable.Result result) {
+    List<String> filesToMove =
+        spark
+            .read()
+            .format("text")
+            .load(result.metadataFileListLocation())
+            .as(Encoders.STRING())
+            .collectAsList();
+    Assert.assertEquals(
+        "The rebuilt version file number should be",
+        versionFileCount,
+        filesToMove.stream().filter(f -> f.endsWith(".metadata.json")).count());
+    Assert.assertEquals(
+        "The rebuilt Manifest list file number should be",
+        manifestListCount,
+        filesToMove.stream().filter(f -> f.contains("snap-")).count());
+    Assert.assertEquals(
+        "The rebuilt Manifest file number should be",
+        manifestFileCount,
+        filesToMove.stream().filter(f -> f.endsWith("-m0.avro")).count());
+  }
+
+  private String stagingDir(CopyTable.Result result) {
+    String metadataFileListPath = result.metadataFileListLocation();
+    return metadataFileListPath.substring(0, metadataFileListPath.lastIndexOf(File.separator));
+  }
+
+  protected String newTableLocation() throws IOException {
+    return temp.newFolder().toURI().toString();
+  }
+
+  private void moveTableFiles(String sourceDir, String targetDir, String stagingDir)
+      throws Exception {
+    FileUtils.copyDirectory(
+        new File(removePrefix(sourceDir) + "data/"), new File(removePrefix(targetDir) + "/data/"));
+    FileUtils.copyDirectory(
+        new File(removePrefix(stagingDir)), new File(removePrefix(targetDir) + "/metadata/"));
+  }
+
+  private String removePrefix(String path) {
+    return path.substring(path.lastIndexOf(":") + 1);
+  }
+
+  // Metastore table tests
+  @Test
+  public void testMetadataLocationChange() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createMetastoreTable(sourceTableLocation, Maps.newHashMap(), "tbl", 1);
+    String metadataFilePath = currentMetadata(sourceTable).metadataFileLocation();
+
+    String newMetadataDir = "new-metadata-dir";
+    sourceTable
+        .updateProperties()
+        .set(TableProperties.WRITE_METADATA_LOCATION, sourceTableLocation + newMetadataDir)
+        .commit();
+
+    spark.sql("insert into hive.default.tbl values (1, 'AAAAAAAAAA', 'AAAA')");
+    sourceTable.refresh();
+
+    // copy table
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .execute();
+
+    checkMetadataFileNum(4, 2, 2, result);
+    checkDataFileNum(2, result);
+
+    // pick up a version from the old metadata dir as the end version
+    CopyTable.Result result1 =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .endVersion(fileName(metadataFilePath))
+            .execute();
+
+    checkMetadataFileNum(2, 1, 1, result1);
+
+    // pick up a version from the old metadata dir as the last copied version
+    CopyTable.Result result2 =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .lastCopiedVersion(fileName(metadataFilePath))
+            .execute();
+
+    checkMetadataFileNum(2, 1, 1, result2);
+  }
+
+  @Test
+  public void testMetadataCompressionWithMetastoreTable() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Map<String, String> properties = Maps.newHashMap();
+    properties.put(TableProperties.METADATA_COMPRESSION, "gzip");
+    Table sourceTable =
+        createMetastoreTable(sourceTableLocation, properties, "testMetadataCompression", 2);
+
+    TableMetadata currentMetadata = currentMetadata(sourceTable);
+
+    // set the second version as the endVersion
+    String endVersion = fileName(currentMetadata.previousFiles().get(1).file());
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .endVersion(endVersion)
+            .execute();
+
+    checkMetadataFileNum(4, result);
+    checkDataFileNum(1, result);
+
+    // set the first version as the lastCopiedVersion
+    String firstVersion = fileName(currentMetadata.previousFiles().get(0).file());
+    result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .lastCopiedVersion(firstVersion)
+            .execute();
+
+    checkMetadataFileNum(6, result);
+    checkDataFileNum(2, result);
+  }
+
+  private TableMetadata currentMetadata(Table tbl) {
+    return ((HasTableOperations) tbl).operations().current();
+  }
+
+  @Test
+  public void testDataFileLocationChange() throws Exception {
+    String sourceTableLocation = newTableLocation();
+    Table sourceTable = createMetastoreTable(sourceTableLocation, Maps.newHashMap(), "tbl1", 1);
+    String metadataFilePath = currentMetadata(sourceTable).metadataFileLocation();
+
+    String newMetadataDir = "new-data-dir";
+    sourceTable
+        .updateProperties()
+        .set(TableProperties.OBJECT_STORE_PATH, sourceTableLocation + newMetadataDir)
+        .set(TableProperties.OBJECT_STORE_ENABLED, "true")
+        .commit();
+
+    spark.sql("insert into hive.default.tbl1 values (1, 'AAAAAAAAAA', 'AAAA')");
+    sourceTable.refresh();
+
+    // copy table
+    CopyTable.Result result =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, newTableLocation())
+            .execute();
+
+    checkMetadataFileNum(4, 2, 2, result);
+    checkDataFileNum(2, result);
+
+    // pick up a version with the data file in the old data directory as the end version
+    String targetTableLocation = newTableLocation();
+    CopyTable.Result result1 =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, targetTableLocation)
+            .endVersion(fileName(metadataFilePath))
+            .execute();
+
+    checkMetadataFileNum(2, 1, 1, result1);
+    checkDataFileNum(1, result1);
+    List<String> filesToMove1 =
+        spark
+            .read()
+            .format("text")
+            .load(result1.dataFileListLocation())
+            .as(Encoders.STRING())
+            .collectAsList();
+    Assert.assertTrue(
+        "The data file should be in the old data directory.",
+        filesToMove1.stream().findFirst().get().startsWith(sourceTableLocation + "data"));
+
+    // pick up a version with the data file in the new data directory as the last copied version
+    CopyTable.Result result2 =
+        actions()
+            .copyTable(sourceTable)
+            .rewriteLocationPrefix(sourceTableLocation, targetTableLocation)
+            .lastCopiedVersion(fileName(metadataFilePath))
+            .execute();
+
+    checkMetadataFileNum(2, 1, 1, result2);
+    checkDataFileNum(1, result2);
+    List<String> filesToMove2 =
+        spark
+            .read()
+            .format("text")
+            .load(result2.dataFileListLocation())
+            .as(Encoders.STRING())
+            .collectAsList();
+    Assert.assertTrue(
+        "The data file should be in the new data directory.",
+        filesToMove2.stream().findFirst().get().startsWith(sourceTableLocation + newMetadataDir));
+
+    // check if table properties have been modified
+    List<String> metadataFilesToMove =
+        spark
+            .read()
+            .format("text")
+            .load(result2.metadataFileListLocation())
+            .as(Encoders.STRING())
+            .collectAsList();
+    metadataFilesToMove.stream()
+        .filter(f -> f.endsWith(".metadata.json"))
+        .forEach(
+            metadataFile -> {
+              StaticTableOperations ops = new StaticTableOperations(metadataFile, sourceTable.io());
+              Table targetStaticTable = new BaseTable(ops, metadataFile);
+              if (targetStaticTable.properties().containsKey(TableProperties.OBJECT_STORE_PATH)) {
+                Assert.assertTrue(
+                    "The write.object-storage.path should be modified with the target table location.",
+                    targetStaticTable
+                        .properties()
+                        .get(TableProperties.OBJECT_STORE_PATH)
+                        .startsWith(targetTableLocation));
+              }
+            });
+  }
+
+  private Table createMetastoreTable(
+      String location, Map<String, String> properties, String tableName, int snapshotNumber) {
+    spark.conf().set("spark.sql.catalog.hive", SparkCatalog.class.getName());
+    spark.conf().set("spark.sql.catalog.hive.type", "hive");
+    spark.conf().set("spark.sql.catalog.hive.default-namespace", "default");
+    spark.conf().set("spark.sql.catalog.hive.cache-enabled", "false");
+
+    StringBuilder propertiesStr = new StringBuilder();
+    properties.forEach((k, v) -> propertiesStr.append("'" + k + "'='" + v + "',"));
+    String tblProperties =
+        propertiesStr.substring(0, propertiesStr.length() > 0 ? propertiesStr.length() - 1 : 0);
+
+    if (tblProperties.isEmpty()) {
+      sql(
+          "CREATE TABLE hive.default.%s (c1 bigint, c2 string, c3 string) USING iceberg LOCATION '%s'",
+          tableName, location);
+    } else {
+      sql(
+          "CREATE TABLE hive.default.%s (c1 bigint, c2 string, c3 string) USING iceberg LOCATION '%s' TBLPROPERTIES "
+              + "(%s)",
+          tableName, location, tblProperties);
+    }
+
+    for (int i = 0; i < snapshotNumber; i++) {
+      sql("insert into hive.default.%s values (1, 'AAAAAAAAAA', 'AAAA')", tableName);
+    }
+    return catalog.loadTable(TableIdentifier.of("default", tableName));
+  }
+
+  private static String fileName(String path) {
+    String filename = path;
+    int lastIndex = path.lastIndexOf(File.separator);
+    if (lastIndex != -1) {
+      filename = path.substring(lastIndex + 1);
+    }
+    return filename;
+  }
+}
diff --git a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveExpiredFilesAction.java b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveExpiredFilesAction.java
new file mode 100644
index 000000000..2d2d109cb
--- /dev/null
+++ b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveExpiredFilesAction.java
@@ -0,0 +1,233 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.iceberg.spark.actions;
+
+import static org.apache.iceberg.types.Types.NestedField.optional;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+import org.apache.commons.io.FileUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.AssertHelpers;
+import org.apache.iceberg.BaseTable;
+import org.apache.iceberg.HasTableOperations;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.StaticTableOperations;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableMetadata;
+import org.apache.iceberg.actions.ActionsProvider;
+import org.apache.iceberg.actions.CheckSnapshotIntegrity;
+import org.apache.iceberg.actions.CopyTable;
+import org.apache.iceberg.actions.RemoveExpiredFiles;
+import org.apache.iceberg.hadoop.HadoopTables;
+import org.apache.iceberg.io.FileIO;
+import org.apache.iceberg.relocated.com.google.common.collect.Lists;
+import org.apache.iceberg.relocated.com.google.common.collect.Maps;
+import org.apache.iceberg.spark.SparkTestBase;
+import org.apache.iceberg.spark.source.ThreeColumnRecord;
+import org.apache.iceberg.types.Types;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Encoders;
+import org.apache.spark.sql.Row;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+public class TestRemoveExpiredFilesAction extends SparkTestBase {
+  private ActionsProvider actions() {
+    return SparkActions.get();
+  }
+
+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());
+  protected static final Schema SCHEMA =
+      new Schema(
+          optional(1, "c1", Types.IntegerType.get()),
+          optional(2, "c2", Types.StringType.get()),
+          optional(3, "c3", Types.StringType.get()));
+
+  @Rule public TemporaryFolder temp = new TemporaryFolder();
+  private File tableDir = null;
+  protected String tableLocation = null;
+  private Table table = null;
+
+  @Before
+  public void setupTableLocation() throws Exception {
+    this.tableDir = temp.newFolder();
+    this.tableLocation = tableDir.toURI().toString();
+    this.table = createTableWith2Snapshots();
+  }
+
+  @Test
+  public void testRemoveExpiredFiles() throws Exception {
+    String tblLocation = newTableLocation();
+    Table tbl = createTableWith2Snapshots(tblLocation);
+    String targetLocation = newTableLocation();
+
+    // copy it to a new place, we have two identical tables after this
+    CopyTable.Result result =
+        actions().copyTable(tbl).rewriteLocationPrefix(tblLocation, targetLocation).execute();
+    moveTableFiles(tblLocation, targetLocation, stagingDir(result));
+    String tgtMetadataLoc = targetLocation + "metadata/" + result.latestVersion();
+    Table targetTable = newStaticTable(tgtMetadataLoc, tbl.io());
+
+    // expire the first snapshot in the source table
+    Table staticTable = newStaticTable(tblLocation + "metadata/v2.metadata.json", tbl.io());
+    actions()
+        .expireSnapshots(tbl)
+        .expireSnapshotId(staticTable.currentSnapshot().snapshotId())
+        .execute();
+
+    // copy it again
+    CopyTable.Result result2 =
+        actions()
+            .copyTable(tbl)
+            .targetTable(targetTable)
+            .rewriteLocationPrefix(tblLocation, targetLocation)
+            .execute();
+    moveTableFiles(tblLocation, targetLocation, stagingDir(result2));
+    String tgtMetadataLocNew = targetLocation + "metadata/" + result2.latestVersion();
+
+    // remove expired files
+    RemoveExpiredFiles.Result integrityResult =
+        actions().removeExpiredFiles(targetTable).targetVersion(tgtMetadataLocNew).execute();
+
+    // one manifest list file is deleted
+    Assert.assertEquals(
+        "Deleted manifest list file count should be",
+        1L,
+        integrityResult.deletedManifestListsCount());
+    Assert.assertEquals(
+        "Deleted data file count should be", 0L, integrityResult.deletedDataFilesCount());
+    Assert.assertEquals(
+        "Deleted manifest file count should be", 0L, integrityResult.deletedManifestsCount());
+
+    // check file count
+    Assert.assertEquals(
+        "Source table file count should equal to target table one",
+        validFiles(tblLocation).size(),
+        validFiles(targetLocation).size());
+
+    // verify data rows
+    Assert.assertEquals("Rows must match", records(tblLocation), records(targetLocation));
+  }
+
+  @Test
+  public void testInputs() {
+    CheckSnapshotIntegrity actions = actions().checkSnapshotIntegrity(table);
+
+    AssertHelpers.assertThrows("", IllegalArgumentException.class, () -> actions.targetVersion(""));
+    AssertHelpers.assertThrows(
+        "", IllegalArgumentException.class, () -> actions.targetVersion(null));
+    AssertHelpers.assertThrows(
+        "", IllegalArgumentException.class, () -> actions.targetVersion("invalid"));
+
+    // either version file name or path are valid
+    String versionFilePath = currentMetadata(table).metadataFileLocation();
+    actions.targetVersion(versionFilePath);
+    String versionFilename = fileName(versionFilePath);
+    actions.targetVersion(versionFilename);
+  }
+
+  private TableMetadata currentMetadata(Table tbl) {
+    return ((HasTableOperations) tbl).operations().current();
+  }
+
+  private static String fileName(String path) {
+    String filename = path;
+    int lastIndex = path.lastIndexOf(File.separator);
+    if (lastIndex != -1) {
+      filename = path.substring(lastIndex + 1);
+    }
+    return filename;
+  }
+
+  private List<ThreeColumnRecord> records(String location) {
+    Dataset<Row> resultDF = spark.read().format("iceberg").load(location);
+    return resultDF
+        .sort("c1", "c2", "c3")
+        .as(Encoders.bean(ThreeColumnRecord.class))
+        .collectAsList();
+  }
+
+  private String stagingDir(CopyTable.Result result) {
+    String metadataFileListPath = result.metadataFileListLocation();
+    return metadataFileListPath.substring(0, metadataFileListPath.lastIndexOf(File.separator));
+  }
+
+  private void moveTableFiles(String sourceDir, String targetDir, String stagingDir)
+      throws Exception {
+    FileUtils.copyDirectory(
+        new File(removePrefix(sourceDir) + "data/"), new File(removePrefix(targetDir) + "/data/"));
+    FileUtils.copyDirectory(
+        new File(removePrefix(stagingDir)), new File(removePrefix(targetDir) + "/metadata/"));
+  }
+
+  private String removePrefix(String path) {
+    return path.substring(path.lastIndexOf(":") + 1);
+  }
+
+  private Table newStaticTable(String metadataFileLocation, FileIO io) {
+    StaticTableOperations ops = new StaticTableOperations(metadataFileLocation, io);
+    return new BaseTable(ops, metadataFileLocation);
+  }
+
+  private List<String> validFiles(String location) {
+    return spark
+        .read()
+        .format("iceberg")
+        .load(location + "#files")
+        .select("file_path")
+        .as(Encoders.STRING())
+        .collectAsList();
+  }
+
+  private Table createTableWith2Snapshots() {
+    return createTableWith2Snapshots(tableLocation);
+  }
+
+  private Table createTableWith2Snapshots(String tblLocation) {
+    return createTableWithSnapshots(tblLocation, 2);
+  }
+
+  private Table createTableWithSnapshots(String tblLocation, int snapshotNumber) {
+    Table tbl =
+        TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tblLocation);
+
+    List<ThreeColumnRecord> records =
+        Lists.newArrayList(new ThreeColumnRecord(1, "AAAAAAAAAA", "AAAA"));
+
+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);
+
+    for (int i = 0; i < snapshotNumber; i++) {
+      df.select("c1", "c2", "c3").write().format("iceberg").mode("append").save(tblLocation);
+    }
+
+    tbl.refresh();
+
+    return tbl;
+  }
+
+  protected String newTableLocation() throws IOException {
+    return temp.newFolder().toURI().toString();
+  }
+}
-- 
2.27.0


From 410951903ee240718348b9d74e20bc1c748be6f8 Mon Sep 17 00:00:00 2001
From: vaultah <4944562+vaultah@users.noreply.github.com>
Date: Thu, 8 Feb 2024 14:53:00 +0000
Subject: [PATCH 2/5] Support for tables with delete manifests

---
 .../org/apache/iceberg/ManifestFiles.java     |   2 +-
 .../actions/BaseCopyTableSparkAction.java     | 312 +++++++++++++++---
 .../spark/actions/TestCopyTableAction.java    |  51 ++-
 3 files changed, 317 insertions(+), 48 deletions(-)

diff --git a/core/src/main/java/org/apache/iceberg/ManifestFiles.java b/core/src/main/java/org/apache/iceberg/ManifestFiles.java
index 3803e07f8..d706b2bf5 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestFiles.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestFiles.java
@@ -101,7 +101,7 @@ public class ManifestFiles {
   public static CloseableIterable<Pair<String, Long>> readPathsWithSnapshotId(
       ManifestFile manifest, FileIO io) {
     return CloseableIterable.transform(
-        read(manifest, io, null).select(ImmutableList.of("file_path", "snapshot_id")).liveEntries(),
+        open(manifest, io, null).select(ImmutableList.of("file_path", "snapshot_id")).liveEntries(),
         entry -> Pair.of(entry.file().path().toString(), entry.snapshotId()));
   }
 
diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
index f68304a14..e1f0e1295 100644
--- a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
@@ -21,14 +21,19 @@ package org.apache.iceberg.spark.actions;
 import java.io.File;
 import java.io.IOException;
 import java.io.UncheckedIOException;
+import java.nio.file.Paths;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 import java.util.UUID;
+import org.apache.iceberg.ContentFile;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DataFiles;
+import org.apache.iceberg.DeleteFile;
+import org.apache.iceberg.FileFormat;
+import org.apache.iceberg.FileMetadata;
 import org.apache.iceberg.HasTableOperations;
 import org.apache.iceberg.ManifestEntry;
 import org.apache.iceberg.ManifestFile;
@@ -37,6 +42,7 @@ import org.apache.iceberg.ManifestLists;
 import org.apache.iceberg.ManifestReader;
 import org.apache.iceberg.ManifestWriter;
 import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.Schema;
 import org.apache.iceberg.SerializableTable;
 import org.apache.iceberg.Snapshot;
 import org.apache.iceberg.StaticTableOperations;
@@ -48,10 +54,26 @@ import org.apache.iceberg.TableMetadataParser;
 import org.apache.iceberg.TableMetadataUtil;
 import org.apache.iceberg.actions.BaseCopyTableActionResult;
 import org.apache.iceberg.actions.CopyTable;
+import org.apache.iceberg.avro.Avro;
+import org.apache.iceberg.data.Record;
+import org.apache.iceberg.data.avro.DataReader;
+import org.apache.iceberg.data.avro.DataWriter;
+import org.apache.iceberg.data.orc.GenericOrcReader;
+import org.apache.iceberg.data.orc.GenericOrcWriter;
+import org.apache.iceberg.data.parquet.GenericParquetReaders;
+import org.apache.iceberg.data.parquet.GenericParquetWriter;
+import org.apache.iceberg.deletes.PositionDelete;
+import org.apache.iceberg.deletes.PositionDeleteWriter;
 import org.apache.iceberg.exceptions.RuntimeIOException;
+import org.apache.iceberg.io.CloseableIterable;
+import org.apache.iceberg.io.CloseableIterator;
+import org.apache.iceberg.io.DeleteSchemaUtil;
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.io.FileIO;
+import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.io.OutputFile;
+import org.apache.iceberg.orc.ORC;
+import org.apache.iceberg.parquet.Parquet;
 import org.apache.iceberg.relocated.com.google.common.base.Preconditions;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 import org.apache.iceberg.relocated.com.google.common.collect.Sets;
@@ -186,8 +208,11 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
     }
 
     if (stagingDir.isEmpty()) {
-      stagingDir = getMetadataLocation(table) + "copy-table-staging-" + UUID.randomUUID() + "/";
-    } else if (!stagingDir.endsWith("/")) {
+      String stagingDirName = "copy-table-staging-" + UUID.randomUUID();
+      stagingDir = Paths.get(table.location(), stagingDirName).toString();
+    }
+
+    if (!stagingDir.endsWith("/")) {
       stagingDir = stagingDir + "/";
     }
   }
@@ -347,7 +372,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
   private Set<Long> rewriteVersionFiles(TableMetadata metadata) {
     Set<Long> allSnapshotIds = Sets.newHashSet();
 
-    String stagingPath = stagingPath(endVersion, stagingDir);
+    String stagingPath = stagingPath(relativize(endVersion, sourcePrefix), stagingDir);
     metadata.snapshots().forEach(snapshot -> allSnapshotIds.add(snapshot.snapshotId()));
     rewriteVersionFile(metadata, stagingPath);
 
@@ -361,7 +386,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
       Preconditions.checkArgument(
           fileExist(versionFilePath),
           String.format("Version file %s doesn't exist", versionFilePath));
-      String newPath = stagingPath(versionFilePath, stagingDir);
+      String newPath = stagingPath(relativize(versionFilePath, sourcePrefix), stagingDir);
       TableMetadata tableMetadata =
           new StaticTableOperations(versionFilePath, table.io()).current();
 
@@ -393,7 +418,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
   private void rewriteManifestList(Snapshot snapshot, TableMetadata tableMetadata) {
     List<ManifestFile> manifestFiles = manifestFilesInSnapshot(snapshot);
     String path = snapshot.manifestListLocation();
-    String stagingPath = stagingPath(path, stagingDir);
+    String stagingPath = stagingPath(relativize(path, sourcePrefix), stagingDir);
     OutputFile outputFile = table.io().newOutputFile(stagingPath);
     try (FileAppender<ManifestFile> writer =
         ManifestLists.write(
@@ -492,28 +517,54 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
     return rows -> {
       List<ManifestFile> manifests = Lists.newArrayList();
       while (rows.hasNext()) {
-        manifests.add(
-            writeManifest(
-                rows.next(), io, stagingLocation, format, specsById, sourcePrefix, targetPrefix));
+        ManifestFile manifestFile = rows.next();
+
+        switch (manifestFile.content()) {
+          case DATA:
+            manifests.add(
+                writeDataManifest(
+                    manifestFile,
+                    stagingLocation,
+                    io,
+                    format,
+                    specsById,
+                    sourcePrefix,
+                    targetPrefix));
+            break;
+          case DELETES:
+            manifests.add(
+                writeDeleteManifest(
+                    manifestFile,
+                    stagingLocation,
+                    io,
+                    format,
+                    specsById,
+                    sourcePrefix,
+                    targetPrefix));
+            break;
+          default:
+            throw new UnsupportedOperationException(
+                "Unsupported manifest type: " + manifestFile.content());
+        }
       }
 
       return manifests.iterator();
     };
   }
 
-  private static ManifestFile writeManifest(
+  private static ManifestFile writeDataManifest(
       ManifestFile manifestFile,
-      Broadcast<FileIO> io,
       String stagingLocation,
+      Broadcast<FileIO> io,
       int format,
       Map<Integer, PartitionSpec> specsById,
       String sourcePrefix,
       String targetPrefix)
       throws IOException {
-
-    String stagingPath = stagingPath(manifestFile.path(), stagingLocation);
-    OutputFile outputFile = io.value().newOutputFile(stagingPath);
     PartitionSpec spec = specsById.get(manifestFile.partitionSpecId());
+    String stagingPath =
+        stagingPath(relativize(manifestFile.path(), sourcePrefix), stagingLocation);
+    OutputFile outputFile = io.value().newOutputFile(stagingPath);
     ManifestWriter<DataFile> writer =
         ManifestFiles.write(format, spec, outputFile, manifestFile.snapshotId());
 
@@ -521,37 +572,214 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
         ManifestFiles.read(manifestFile, io.getValue(), specsById).select(Arrays.asList("*"))) {
       reader
           .entries()
-          .forEach(entry -> appendEntry(entry, writer, spec, sourcePrefix, targetPrefix));
+          .forEach(
+              entry ->
+                  appendEntryWithFile(
+                      entry, writer, newDataFile(entry.file(), spec, sourcePrefix, targetPrefix)));
     } finally {
       writer.close();
     }
+    return writer.toManifestFile();
+  }
+
+  private static DataFile newDataFile(
+      DataFile file, PartitionSpec spec, String sourcePrefix, String targetPrefix) {
+    DataFile transformedFile = file;
+    String filePath = file.path().toString();
+    if (filePath.startsWith(sourcePrefix)) {
+      filePath = newPath(filePath, sourcePrefix, targetPrefix);
+      transformedFile = DataFiles.builder(spec).copy(file).withPath(filePath).build();
+    }
+    return transformedFile;
+  }
+
+  private static ManifestFile writeDeleteManifest(
+      ManifestFile manifestFile,
+      String stagingLocation,
+      Broadcast<FileIO> io,
+      int format,
+      Map<Integer, PartitionSpec> specsById,
+      String sourcePrefix,
+      String targetPrefix)
+      throws IOException {
+    PartitionSpec spec = specsById.get(manifestFile.partitionSpecId());
 
+    String manifestStagingPath =
+        stagingPath(relativize(manifestFile.path(), sourcePrefix), stagingLocation);
+    OutputFile manifestOutputFile = io.value().newOutputFile(manifestStagingPath);
+    ManifestWriter<DeleteFile> writer =
+        ManifestFiles.writeDeleteManifest(
+            format, spec, manifestOutputFile, manifestFile.snapshotId());
+
+    try (ManifestReader<DeleteFile> reader =
+        ManifestFiles.readDeleteManifest(manifestFile, io.getValue(), specsById)
+            .select(Arrays.asList("*"))) {
+
+      for (ManifestEntry<DeleteFile> entry : reader.entries()) {
+        DeleteFile file = entry.file();
+
+        switch (file.content()) {
+          case POSITION_DELETES:
+            String deleteFileStagingPath =
+                stagingPath(relativize(file.path().toString(), sourcePrefix), stagingLocation);
+            rewritePositionDeleteFile(
+                io, file, deleteFileStagingPath, spec, sourcePrefix, targetPrefix);
+            appendEntryWithFile(
+                entry, writer, newDeleteFile(file, spec, sourcePrefix, targetPrefix));
+            break;
+          case EQUALITY_DELETES:
+            appendEntryWithFile(
+                entry, writer, newDeleteFile(file, spec, sourcePrefix, targetPrefix));
+            break;
+          default:
+            throw new UnsupportedOperationException(
+                "Unsupported delete file type: " + file.content());
+        }
+      }
+    } finally {
+      writer.close();
+    }
     return writer.toManifestFile();
   }
 
-  private static void appendEntry(
-      ManifestEntry<DataFile> entry,
-      ManifestWriter<DataFile> writer,
+  private static DeleteFile newDeleteFile(
+      DeleteFile file, PartitionSpec spec, String sourcePrefix, String targetPrefix) {
+    DeleteFile transformedFile = file;
+    String filePath = file.path().toString();
+    if (filePath.startsWith(sourcePrefix)) {
+      filePath = newPath(filePath, sourcePrefix, targetPrefix);
+      transformedFile = FileMetadata.deleteFileBuilder(spec).copy(file).withPath(filePath).build();
+    }
+    return transformedFile;
+  }
+
+  private static PositionDelete newPositionDeleteRecord(
+      Record record, String sourcePrefix, String targetPrefix) {
+    PositionDelete delete = PositionDelete.create();
+    delete.set(
+        newPath((String) record.get(0), sourcePrefix, targetPrefix),
+        (Long) record.get(1),
+        record.get(2));
+    return delete;
+  }
+
+  private static DeleteFile rewritePositionDeleteFile(
+      Broadcast<FileIO> io,
+      DeleteFile current,
+      String path,
       PartitionSpec spec,
       String sourcePrefix,
-      String targetPrefix) {
-    DataFile dataFile = entry.file();
-    String dataFilePath = dataFile.path().toString();
-    if (dataFilePath.startsWith(sourcePrefix)) {
-      dataFilePath = newPath(dataFilePath, sourcePrefix, targetPrefix);
-      dataFile = DataFiles.builder(spec).copy(entry.file()).withPath(dataFilePath).build();
+      String targetPrefix)
+      throws IOException {
+    OutputFile targetFile = io.value().newOutputFile(path);
+    InputFile sourceFile = io.value().newInputFile(current.path().toString());
+
+    try (CloseableIterable<Record> reader =
+        positionDeletesReader(sourceFile, current.format(), spec)) {
+      Record record = null;
+      Schema rowSchema = null;
+      CloseableIterator<Record> recordIt = reader.iterator();
+
+      if (recordIt.hasNext()) {
+        record = recordIt.next();
+        rowSchema = record.get(2) != null ? spec.schema() : null;
+      }
+
+      PositionDeleteWriter<Record> writer =
+          positionDeletesWriter(targetFile, current.format(), spec, current.partition(), rowSchema);
+
+      try {
+        if (record != null) {
+          writer.write(newPositionDeleteRecord(record, sourcePrefix, targetPrefix));
+        }
+
+        while (recordIt.hasNext()) {
+          record = recordIt.next();
+          writer.write(newPositionDeleteRecord(record, sourcePrefix, targetPrefix));
+        }
+      } finally {
+        writer.close();
+      }
+      return writer.toDeleteFile();
+    }
+  }
+
+  private static CloseableIterable<Record> positionDeletesReader(
+      InputFile inputFile, FileFormat format, PartitionSpec spec) throws IOException {
+    Schema deleteSchema = DeleteSchemaUtil.posDeleteSchema(spec.schema());
+    switch (format) {
+      case AVRO:
+        return Avro.read(inputFile)
+            .project(deleteSchema)
+            .reuseContainers()
+            .createReaderFunc(DataReader::create)
+            .build();
+
+      case PARQUET:
+        return Parquet.read(inputFile)
+            .project(deleteSchema)
+            .reuseContainers()
+            .createReaderFunc(
+                fileSchema -> GenericParquetReaders.buildReader(deleteSchema, fileSchema))
+            .build();
+
+      case ORC:
+        return ORC.read(inputFile)
+            .project(deleteSchema)
+            .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(deleteSchema, fileSchema))
+            .build();
+
+      default:
+        throw new UnsupportedOperationException("Unsupported file format: " + format);
+    }
+  }
+
+  private static PositionDeleteWriter<Record> positionDeletesWriter(
+      OutputFile outputFile,
+      FileFormat format,
+      PartitionSpec spec,
+      StructLike partition,
+      Schema rowSchema)
+      throws IOException {
+    switch (format) {
+      case AVRO:
+        return Avro.writeDeletes(outputFile)
+            .createWriterFunc(DataWriter::create)
+            .withPartition(partition)
+            .rowSchema(rowSchema)
+            .withSpec(spec)
+            .buildPositionWriter();
+      case PARQUET:
+        return Parquet.writeDeletes(outputFile)
+            .createWriterFunc(GenericParquetWriter::buildWriter)
+            .withPartition(partition)
+            .rowSchema(rowSchema)
+            .withSpec(spec)
+            .buildPositionWriter();
+      case ORC:
+        return ORC.writeDeletes(outputFile)
+            .createWriterFunc(GenericOrcWriter::buildWriter)
+            .withPartition(partition)
+            .rowSchema(rowSchema)
+            .withSpec(spec)
+            .buildPositionWriter();
+      default:
+        throw new UnsupportedOperationException("Unsupported file format: " + format);
     }
+  }
 
+  private static <F extends ContentFile<F>> void appendEntryWithFile(
+      ManifestEntry<F> entry, ManifestWriter<F> writer, F file) {
     switch (entry.status()) {
       case ADDED:
-        writer.add(dataFile);
+        writer.add(file);
         break;
       case EXISTING:
         writer.existing(
-            dataFile, entry.snapshotId(), entry.dataSequenceNumber(), entry.fileSequenceNumber());
+            file, entry.snapshotId(), entry.dataSequenceNumber(), entry.fileSequenceNumber());
         break;
       case DELETED:
-        writer.delete(dataFile, entry.dataSequenceNumber(), entry.fileSequenceNumber());
+        writer.delete(file, entry.dataSequenceNumber(), entry.fileSequenceNumber());
         break;
     }
   }
@@ -579,18 +807,26 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
     return table.io().newInputFile(path).exists();
   }
 
+  private static String relativize(String path, String prefix) {
+    if (!path.startsWith(prefix)) {
+      throw new IllegalArgumentException(
+          String.format("Path %s does not start with %s", path, prefix));
+    }
+    return path.replaceFirst(prefix, "");
+  }
+
+  private static String stagingPath(String originalPath, String stagingLocation) {
+    return Paths.get(stagingLocation, originalPath).toString();
+  }
+
   private static String newPath(String path, String sourcePrefix, String targetPrefix) {
-    return path.replaceFirst(sourcePrefix, targetPrefix);
+    return Paths.get(targetPrefix, relativize(path, sourcePrefix)).toString();
   }
 
   private void addToRebuiltFiles(String path) {
     metadataFilesToMove.add(path);
   }
 
-  private static String stagingPath(String originalPath, String stagingLocation) {
-    return stagingLocation + fileName(originalPath);
-  }
-
   private String currentMetadataPath(Table tbl) {
     return ((HasTableOperations) tbl).operations().current().metadataFileLocation();
   }
@@ -603,18 +839,4 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
     }
     return filename;
   }
-
-  private String getMetadataLocation(Table tbl) {
-    String currentMetadataPath =
-        ((HasTableOperations) tbl).operations().current().metadataFileLocation();
-    int lastIndex = currentMetadataPath.lastIndexOf(File.separator);
-    String metadataDir = "";
-    if (lastIndex != -1) {
-      metadataDir = currentMetadataPath.substring(0, lastIndex + 1);
-    }
-
-    Preconditions.checkArgument(
-        !metadataDir.isEmpty(), "Failed to get the metadata file root directory");
-    return metadataDir;
-  }
 }
diff --git a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
index 7d44ff0e3..f79b04c5b 100644
--- a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
+++ b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
@@ -28,6 +28,7 @@ import org.apache.commons.io.FileUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.AssertHelpers;
 import org.apache.iceberg.BaseTable;
+import org.apache.iceberg.DeleteFile;
 import org.apache.iceberg.HasTableOperations;
 import org.apache.iceberg.PartitionSpec;
 import org.apache.iceberg.Schema;
@@ -38,6 +39,7 @@ import org.apache.iceberg.TableProperties;
 import org.apache.iceberg.actions.ActionsProvider;
 import org.apache.iceberg.actions.CopyTable;
 import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.data.FileHelpers;
 import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -46,6 +48,7 @@ import org.apache.iceberg.spark.SparkCatalog;
 import org.apache.iceberg.spark.SparkTestBase;
 import org.apache.iceberg.spark.source.ThreeColumnRecord;
 import org.apache.iceberg.types.Types;
+import org.apache.iceberg.util.Pair;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Encoders;
 import org.apache.spark.sql.Row;
@@ -264,6 +267,50 @@ public class TestCopyTableAction extends SparkTestBase {
         resultDF.as(Encoders.bean(ThreeColumnRecord.class)).count());
   }
 
+  @Test
+  public void testWithDeleteManifests() throws Exception {
+    String location = newTableLocation();
+    Table sourceTable = createATableWith2Snapshots(location);
+    String targetLocation = newTableLocation();
+
+    List<Pair<CharSequence, Long>> deletes =
+        Lists.newArrayList(
+            Pair.of(
+                sourceTable
+                    .currentSnapshot()
+                    .addedDataFiles(sourceTable.io())
+                    .iterator()
+                    .next()
+                    .path(),
+                0L));
+
+    File file = new File(removePrefix(sourceTable.location()) + "/data/deeply/nested/file.parquet");
+    DeleteFile positionDeletes =
+        FileHelpers.writeDeleteFile(
+                table, sourceTable.io().newOutputFile(file.toURI().toString()), deletes)
+            .first();
+
+    Dataset<Row> resultDF = spark.read().format("iceberg").load(location);
+    sourceTable.newRowDelta().addDeletes(positionDeletes).commit();
+
+    CopyTable.Result result =
+        actions().copyTable(sourceTable).rewriteLocationPrefix(location, targetLocation).execute();
+
+    // We have one more snapshot, an additional manifest list, and a new (delete) manifest
+    checkMetadataFileNum(4, 3, 3, result);
+    // We have one additional file for positional deletes
+    checkDataFileNum(3, result);
+
+    // copy the metadata files and data files
+    moveTableFiles(location, targetLocation, stagingDir(result));
+
+    // Positional delete affects a single row, so only one row must remain
+    Assert.assertEquals(
+        "The number of rows should be",
+        1,
+        spark.read().format("iceberg").load(targetLocation).count());
+  }
+
   @Test
   public void testFullTableCopyWithDeletedVersionFiles() throws Exception {
     String location = newTableLocation();
@@ -690,8 +737,8 @@ public class TestCopyTableAction extends SparkTestBase {
       throws Exception {
     FileUtils.copyDirectory(
         new File(removePrefix(sourceDir) + "data/"), new File(removePrefix(targetDir) + "/data/"));
-    FileUtils.copyDirectory(
-        new File(removePrefix(stagingDir)), new File(removePrefix(targetDir) + "/metadata/"));
+    // Copy staged metadata files, overwrite previously copied data files with staged ones
+    FileUtils.copyDirectory(new File(removePrefix(stagingDir)), new File(removePrefix(targetDir)));
   }
 
   private String removePrefix(String path) {
-- 
2.27.0


From 6d1921c06f9b4e7e857e824a4363ccaefd10cd97 Mon Sep 17 00:00:00 2001
From: vaultah <4944562+vaultah@users.noreply.github.com>
Date: Wed, 14 Feb 2024 16:33:40 +0000
Subject: [PATCH 3/5] Improve path operations

---
 .../org/apache/iceberg/actions/CopyTable.java |  3 ++
 .../actions/BaseCopyTableActionResult.java    | 11 ++++-
 .../actions/BaseCopyTableSparkAction.java     | 40 ++++++++++---------
 3 files changed, 34 insertions(+), 20 deletions(-)

diff --git a/api/src/main/java/org/apache/iceberg/actions/CopyTable.java b/api/src/main/java/org/apache/iceberg/actions/CopyTable.java
index 245fec871..33f040dd5 100644
--- a/api/src/main/java/org/apache/iceberg/actions/CopyTable.java
+++ b/api/src/main/java/org/apache/iceberg/actions/CopyTable.java
@@ -76,6 +76,9 @@ public interface CopyTable extends Action<CopyTable, CopyTable.Result> {
 
   /** The action result that contains a summary of the execution. */
   interface Result {
+    /** Return staging location */
+    String stagingLocation();
+
     /** Return directory of data files list. */
     String dataFileListLocation();
 
diff --git a/core/src/main/java/org/apache/iceberg/actions/BaseCopyTableActionResult.java b/core/src/main/java/org/apache/iceberg/actions/BaseCopyTableActionResult.java
index 060b341f0..b29673136 100644
--- a/core/src/main/java/org/apache/iceberg/actions/BaseCopyTableActionResult.java
+++ b/core/src/main/java/org/apache/iceberg/actions/BaseCopyTableActionResult.java
@@ -19,17 +19,24 @@
 package org.apache.iceberg.actions;
 
 public class BaseCopyTableActionResult implements CopyTable.Result {
-  private final String latestVersion;
+  private final String stagingDirPath;
   private final String dataFileListPath;
   private final String metadataFileListPath;
+  private final String latestVersion;
 
   public BaseCopyTableActionResult(
-      String dataFileListPath, String metadataFileListPath, String latestVersion) {
+      String stagingDirPath, String dataFileListPath, String metadataFileListPath, String latestVersion) {
+    this.stagingDirPath = stagingDirPath;
     this.dataFileListPath = dataFileListPath;
     this.metadataFileListPath = metadataFileListPath;
     this.latestVersion = latestVersion;
   }
 
+  @Override
+  public String stagingLocation() {
+    return stagingDirPath;
+  }
+
   @Override
   public String dataFileListLocation() {
     return dataFileListPath;
diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
index e1f0e1295..c9d616d47 100644
--- a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
@@ -21,7 +21,6 @@ package org.apache.iceberg.spark.actions;
 import java.io.File;
 import java.io.IOException;
 import java.io.UncheckedIOException;
-import java.nio.file.Paths;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
@@ -184,7 +183,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
   private CopyTable.Result doExecute() {
     rebuildMetaData();
     return new BaseCopyTableActionResult(
-        dataFileListPath, metadataFileListPath, fileName(endVersion));
+        stagingDir, dataFileListPath, metadataFileListPath, fileName(endVersion));
   }
 
   private void validateInputs() {
@@ -209,7 +208,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
 
     if (stagingDir.isEmpty()) {
       String stagingDirName = "copy-table-staging-" + UUID.randomUUID();
-      stagingDir = Paths.get(table.location(), stagingDirName).toString();
+      stagingDir = combinePaths(table.location(), stagingDirName);
     }
 
     if (!stagingDir.endsWith("/")) {
@@ -372,7 +371,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
   private Set<Long> rewriteVersionFiles(TableMetadata metadata) {
     Set<Long> allSnapshotIds = Sets.newHashSet();
 
-    String stagingPath = stagingPath(relativize(endVersion, sourcePrefix), stagingDir);
+    String stagingPath = combinePaths(stagingDir, relativize(endVersion, sourcePrefix));
     metadata.snapshots().forEach(snapshot -> allSnapshotIds.add(snapshot.snapshotId()));
     rewriteVersionFile(metadata, stagingPath);
 
@@ -386,7 +385,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
       Preconditions.checkArgument(
           fileExist(versionFilePath),
           String.format("Version file %s doesn't exist", versionFilePath));
-      String newPath = stagingPath(relativize(versionFilePath, sourcePrefix), stagingDir);
+      String newPath = combinePaths(stagingDir, relativize(versionFilePath, sourcePrefix));
       TableMetadata tableMetadata =
           new StaticTableOperations(versionFilePath, table.io()).current();
 
@@ -418,7 +417,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
   private void rewriteManifestList(Snapshot snapshot, TableMetadata tableMetadata) {
     List<ManifestFile> manifestFiles = manifestFilesInSnapshot(snapshot);
     String path = snapshot.manifestListLocation();
-    String stagingPath = stagingPath(relativize(path, sourcePrefix), stagingDir);
+    String stagingPath = combinePaths(stagingDir, relativize(path, sourcePrefix));
     OutputFile outputFile = table.io().newOutputFile(stagingPath);
     try (FileAppender<ManifestFile> writer =
         ManifestLists.write(
@@ -524,8 +523,8 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
             manifests.add(
                 writeDataManifest(
                     manifestFile,
-                    stagingLocation,
                     io,
+                    stagingLocation,
                     format,
                     specsById,
                     sourcePrefix,
@@ -535,8 +534,8 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
             manifests.add(
                 writeDeleteManifest(
                     manifestFile,
-                    stagingLocation,
                     io,
+                    stagingLocation,
                     format,
                     specsById,
                     sourcePrefix,
@@ -554,8 +553,8 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
 
   private static ManifestFile writeDataManifest(
       ManifestFile manifestFile,
-      String stagingLocation,
       Broadcast<FileIO> io,
+      String stagingLocation,
       int format,
       Map<Integer, PartitionSpec> specsById,
       String sourcePrefix,
@@ -563,7 +562,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
       throws IOException {
     PartitionSpec spec = specsById.get(manifestFile.partitionSpecId());
     String stagingPath =
-        stagingPath(relativize(manifestFile.path(), sourcePrefix), stagingLocation);
+        combinePaths(stagingLocation, relativize(manifestFile.path(), sourcePrefix));
     OutputFile outputFile = io.value().newOutputFile(stagingPath);
     ManifestWriter<DataFile> writer =
         ManifestFiles.write(format, spec, outputFile, manifestFile.snapshotId());
@@ -595,8 +594,8 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
 
   private static ManifestFile writeDeleteManifest(
       ManifestFile manifestFile,
-      String stagingLocation,
       Broadcast<FileIO> io,
+      String stagingLocation,
       int format,
       Map<Integer, PartitionSpec> specsById,
       String sourcePrefix,
@@ -605,7 +604,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
     PartitionSpec spec = specsById.get(manifestFile.partitionSpecId());
 
     String manifestStagingPath =
-        stagingPath(relativize(manifestFile.path(), sourcePrefix), stagingLocation);
+        combinePaths(stagingLocation, relativize(manifestFile.path(), sourcePrefix));
     OutputFile manifestOutputFile = io.value().newOutputFile(manifestStagingPath);
     ManifestWriter<DeleteFile> writer =
         ManifestFiles.writeDeleteManifest(
@@ -621,7 +620,7 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
         switch (file.content()) {
           case POSITION_DELETES:
             String deleteFileStagingPath =
-                stagingPath(relativize(file.path().toString(), sourcePrefix), stagingLocation);
+                combinePaths(stagingLocation, relativize(file.path().toString(), sourcePrefix));
             rewritePositionDeleteFile(
                 io, file, deleteFileStagingPath, spec, sourcePrefix, targetPrefix);
             appendEntryWithFile(
@@ -815,12 +814,8 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
     return path.replaceFirst(prefix, "");
   }
 
-  private static String stagingPath(String originalPath, String stagingLocation) {
-    return Paths.get(stagingLocation, originalPath).toString();
-  }
-
   private static String newPath(String path, String sourcePrefix, String targetPrefix) {
-    return Paths.get(targetPrefix, relativize(path, sourcePrefix)).toString();
+    return combinePaths(targetPrefix, relativize(path, sourcePrefix));
   }
 
   private void addToRebuiltFiles(String path) {
@@ -831,6 +826,15 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
     return ((HasTableOperations) tbl).operations().current().metadataFileLocation();
   }
 
+  private static String combinePaths(String absolutePath, String relativePath) {
+    String combined = absolutePath;
+    if (!combined.endsWith("/")) {
+      combined += "/";
+    }
+    combined += relativePath;
+    return combined;
+  }
+
   private static String fileName(String path) {
     String filename = path;
     int lastIndex = path.lastIndexOf(File.separator);
-- 
2.27.0


From f92149dbefc45f02b36071e4768c55e301c895f5 Mon Sep 17 00:00:00 2001
From: vaultah <4944562+vaultah@users.noreply.github.com>
Date: Wed, 14 Feb 2024 23:13:01 +0000
Subject: [PATCH 4/5] Improve custom relativize

---
 .../spark/actions/BaseCopyTableSparkAction.java        | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
index c9d616d47..33c02354b 100644
--- a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
@@ -807,11 +807,15 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
   }
 
   private static String relativize(String path, String prefix) {
-    if (!path.startsWith(prefix)) {
+    String toRemove = prefix;
+    if (!toRemove.endsWith("/")) {
+      toRemove += "/";
+    }
+    if (!path.startsWith(toRemove)) {
       throw new IllegalArgumentException(
-          String.format("Path %s does not start with %s", path, prefix));
+          String.format("Path %s does not start with %s", path, toRemove));
     }
-    return path.replaceFirst(prefix, "");
+    return path.substring(toRemove.length());
   }
 
   private static String newPath(String path, String sourcePrefix, String targetPrefix) {
-- 
2.27.0


From aabefc03f85b5e01ffe64eb152e01be2c5389ae2 Mon Sep 17 00:00:00 2001
From: vaultah <4944562+vaultah@users.noreply.github.com>
Date: Thu, 15 Feb 2024 03:50:49 +0000
Subject: [PATCH 5/5] Tests for equality deletes

---
 .../actions/BaseCopyTableSparkAction.java     | 38 ++++++++---
 .../spark/actions/TestCopyTableAction.java    | 64 ++++++++++++++++++-
 2 files changed, 90 insertions(+), 12 deletions(-)

diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
index 33c02354b..3c21f2c4a 100644
--- a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/BaseCopyTableSparkAction.java
@@ -517,7 +517,6 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
       List<ManifestFile> manifests = Lists.newArrayList();
       while (rows.hasNext()) {
         ManifestFile manifestFile = rows.next();
-
         switch (manifestFile.content()) {
           case DATA:
             manifests.add(
@@ -619,16 +618,28 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
 
         switch (file.content()) {
           case POSITION_DELETES:
+            String filePath = file.path().toString();
             String deleteFileStagingPath =
-                combinePaths(stagingLocation, relativize(file.path().toString(), sourcePrefix));
-            rewritePositionDeleteFile(
-                io, file, deleteFileStagingPath, spec, sourcePrefix, targetPrefix);
-            appendEntryWithFile(
-                entry, writer, newDeleteFile(file, spec, sourcePrefix, targetPrefix));
+                combinePaths(stagingLocation, relativize(filePath, sourcePrefix));
+            DeleteFile newDeleteFile =
+                rewritePositionDeleteFile(
+                    io, file, deleteFileStagingPath, spec, sourcePrefix, targetPrefix);
+
+            if (filePath.startsWith(sourcePrefix)) {
+              filePath = newPath(filePath, sourcePrefix, targetPrefix);
+              newDeleteFile =
+                  FileMetadata.deleteFileBuilder(spec)
+                      .ofPositionDeletes()
+                      .copy(newDeleteFile)
+                      .withPath(filePath)
+                      .withSplitOffsets(file.splitOffsets())
+                      .build();
+            }
+            appendEntryWithFile(entry, writer, newDeleteFile);
             break;
           case EQUALITY_DELETES:
             appendEntryWithFile(
-                entry, writer, newDeleteFile(file, spec, sourcePrefix, targetPrefix));
+                entry, writer, newEqualityDeleteFile(file, spec, sourcePrefix, targetPrefix));
             break;
           default:
             throw new UnsupportedOperationException(
@@ -641,13 +652,22 @@ public class BaseCopyTableSparkAction extends BaseSparkAction<CopyTable> impleme
     return writer.toManifestFile();
   }
 
-  private static DeleteFile newDeleteFile(
+  private static DeleteFile newEqualityDeleteFile(
       DeleteFile file, PartitionSpec spec, String sourcePrefix, String targetPrefix) {
     DeleteFile transformedFile = file;
     String filePath = file.path().toString();
+
     if (filePath.startsWith(sourcePrefix)) {
+      int[] equalityFieldIds =
+          file.equalityFieldIds().stream().mapToInt(Integer::intValue).toArray();
       filePath = newPath(filePath, sourcePrefix, targetPrefix);
-      transformedFile = FileMetadata.deleteFileBuilder(spec).copy(file).withPath(filePath).build();
+      transformedFile =
+          FileMetadata.deleteFileBuilder(spec)
+              .ofEqualityDeletes(equalityFieldIds)
+              .copy(file)
+              .withPath(filePath)
+              .withSplitOffsets(file.splitOffsets())
+              .build();
     }
     return transformedFile;
   }
diff --git a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
index f79b04c5b..6d0807dd4 100644
--- a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
+++ b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestCopyTableAction.java
@@ -36,10 +36,13 @@ import org.apache.iceberg.StaticTableOperations;
 import org.apache.iceberg.Table;
 import org.apache.iceberg.TableMetadata;
 import org.apache.iceberg.TableProperties;
+import org.apache.iceberg.TestHelpers;
 import org.apache.iceberg.actions.ActionsProvider;
 import org.apache.iceberg.actions.CopyTable;
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.data.FileHelpers;
+import org.apache.iceberg.data.GenericRecord;
+import org.apache.iceberg.data.Record;
 import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
@@ -268,7 +271,7 @@ public class TestCopyTableAction extends SparkTestBase {
   }
 
   @Test
-  public void testWithDeleteManifests() throws Exception {
+  public void testWithDeleteManifestsAndPositionDeletes() throws Exception {
     String location = newTableLocation();
     Table sourceTable = createATableWith2Snapshots(location);
     String targetLocation = newTableLocation();
@@ -287,10 +290,9 @@ public class TestCopyTableAction extends SparkTestBase {
     File file = new File(removePrefix(sourceTable.location()) + "/data/deeply/nested/file.parquet");
     DeleteFile positionDeletes =
         FileHelpers.writeDeleteFile(
-                table, sourceTable.io().newOutputFile(file.toURI().toString()), deletes)
+                sourceTable, sourceTable.io().newOutputFile(file.toURI().toString()), deletes)
             .first();
 
-    Dataset<Row> resultDF = spark.read().format("iceberg").load(location);
     sourceTable.newRowDelta().addDeletes(positionDeletes).commit();
 
     CopyTable.Result result =
@@ -311,6 +313,62 @@ public class TestCopyTableAction extends SparkTestBase {
         spark.read().format("iceberg").load(targetLocation).count());
   }
 
+  @Test
+  public void testWithDeleteManifestsAndEqualityDeletes() throws Exception {
+    String location = newTableLocation();
+    Table sourceTable = createTableWithSnapshots(location, 1);
+    String targetLocation = newTableLocation();
+
+    // Add more varied data
+    List<ThreeColumnRecord> records =
+        Lists.newArrayList(
+            new ThreeColumnRecord(2, "AAAAAAAAAA", "AAAA"),
+            new ThreeColumnRecord(3, "BBBBBBBBBB", "BBBB"),
+            new ThreeColumnRecord(4, "CCCCCCCCCC", "CCCC"),
+            new ThreeColumnRecord(5, "DDDDDDDDDD", "DDDD"));
+    spark
+        .createDataFrame(records, ThreeColumnRecord.class)
+        .coalesce(1)
+        .select("c1", "c2", "c3")
+        .write()
+        .format("iceberg")
+        .mode("append")
+        .save(location);
+
+    Schema deleteRowSchema = sourceTable.schema().select("c2");
+    Record dataDelete = GenericRecord.create(deleteRowSchema);
+    List<Record> dataDeletes =
+        Lists.newArrayList(
+            dataDelete.copy("c2", "AAAAAAAAAA"), dataDelete.copy("c2", "CCCCCCCCCC"));
+    File file = new File(removePrefix(sourceTable.location()) + "/data/deeply/nested/file.parquet");
+    DeleteFile equalityDeletes =
+        FileHelpers.writeDeleteFile(
+            sourceTable,
+            sourceTable.io().newOutputFile(file.toURI().toString()),
+            TestHelpers.Row.of(0),
+            dataDeletes,
+            deleteRowSchema);
+    sourceTable.newRowDelta().addDeletes(equalityDeletes).commit();
+
+    CopyTable.Result result =
+        actions().copyTable(sourceTable).rewriteLocationPrefix(location, targetLocation).execute();
+
+    // We have four metadata files: for the table creation, for the initial snapshot, for the
+    // second append here, and for commit with equality deletes. Thus, we have three manifest lists
+    checkMetadataFileNum(4, 3, 3, result);
+    // A data file for each snapshot (two with data, one with equality deletes)
+    checkDataFileNum(3, result);
+
+    // copy the metadata files and data files
+    moveTableFiles(location, targetLocation, stagingDir(result));
+
+    // Equality deletes affect three rows, so just two rows must remain
+    Assert.assertEquals(
+        "The number of rows should be",
+        2,
+        spark.read().format("iceberg").load(targetLocation).count());
+  }
+
   @Test
   public void testFullTableCopyWithDeletedVersionFiles() throws Exception {
     String location = newTableLocation();
-- 
2.27.0

